{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first.W\n",
      "Gradient check passed!\n",
      "Checking gradient for first.B\n",
      "Gradient check passed!\n",
      "Checking gradient for second.W\n",
      "Gradient check passed!\n",
      "Checking gradient for second.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for first.W\n",
      "Gradient check passed!\n",
      "Checking gradient for first.B\n",
      "Gradient check passed!\n",
      "Checking gradient for second.W\n",
      "Gradient check passed!\n",
      "Checking gradient for second.B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.311214, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308780, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296621, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292648, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293156, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306315, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303455, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305307, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292604, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301709, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303518, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291784, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290269, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300658, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306311, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301759, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301581, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 10, reg = 1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb2949c95b0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.324053, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309732, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310584, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317094, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310904, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259751, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283866, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303755, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290441, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253904, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305014, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264040, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274150, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262197, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.248908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268698, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211230, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234388, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300826, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.322915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301034, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313590, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.310595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300497, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.312325, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304446, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291761, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223468, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294811, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293785, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256296, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274300, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.343580, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234569, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229679, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331034, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.312030, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.298806, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.316767, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275284, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.235005, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.241721, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.234181, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.130812, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.935888, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.036105, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.151130, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.904542, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.777683, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.459859, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.856549, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.591242, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.446960, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.640266, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.434034, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.453823, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.890054, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.534175, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.016953, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.498495, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.449990, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.640400, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.475530, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.236142, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.642196, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.898670, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.471994, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.728648, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.481161, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.357216, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.593017, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.647762, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.749890, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.287260, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 2.046783, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.757525, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.404335, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.545138, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.385939, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.627990, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.938536, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.298413, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.633526, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.813818, Train accuracy: 0.666667, val accuracy: 0.133333\n",
      "Loss: 1.911458, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.271117, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.704519, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.501062, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.789423, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.434986, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.374032, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.729364, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.044922, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.098811, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.805430, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.434874, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.390798, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.017260, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.399639, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.687904, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.703587, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.667389, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.513383, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.373791, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.614565, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.354238, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.682028, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.031399, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.946767, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.787476, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.356164, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.014460, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.608940, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.209654, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.693945, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.621076, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.285763, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.288072, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.295755, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.493178, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.154266, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.392176, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.357701, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.877952, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.915772, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.596331, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.310707, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.820026, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.479578, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.381261, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.590139, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.185417, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.866011, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.325367, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.284802, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.323625, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.542864, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.222715, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.269280, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.637127, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.073403, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.303852, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.363763, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.332995, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.609105, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.396103, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.616175, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.419697, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.401897, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.426724, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.306737, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.296793, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.411813, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.578246, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.590788, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.460206, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.082586, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.519219, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.079066, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.381812, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.493171, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.239973, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.131575, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.249765, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.939038, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.249937, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.275471, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.289598, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.262300, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.372279, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.523069, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.995664, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.329195, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.238317, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.250273, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.460756, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.385090, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.231673, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.488615, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.319524, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.403387, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.226137, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.526549, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.307699, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.585880, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.332296, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.281680, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.200295, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.084361, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.302863, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.445469, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.082006, Train accuracy: 0.533333, val accuracy: 0.133333\n",
      "Loss: 2.126274, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 0.949141, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 0.305236, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.597530, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.252697, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.055581, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.054072, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.075810, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.065272, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.070536, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.068269, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.067260, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.068987, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 200, reg = 1e-3)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=0.1, reg_strength=0.001, n_layers=100\n",
      "Loss: 1.700522, Train accuracy: 0.361889, val accuracy: 0.378000\n",
      "Loss: 1.547436, Train accuracy: 0.497111, val accuracy: 0.478000\n",
      "Loss: 1.877184, Train accuracy: 0.585444, val accuracy: 0.559000\n",
      "Loss: 2.009012, Train accuracy: 0.572000, val accuracy: 0.545000\n",
      "Loss: 1.393621, Train accuracy: 0.654000, val accuracy: 0.608000\n",
      "Loss: 1.605373, Train accuracy: 0.566222, val accuracy: 0.570000\n",
      "Loss: 1.480441, Train accuracy: 0.628222, val accuracy: 0.614000\n",
      "Loss: 1.742828, Train accuracy: 0.576556, val accuracy: 0.552000\n",
      "Loss: 1.553450, Train accuracy: 0.620111, val accuracy: 0.583000\n",
      "Loss: 1.300220, Train accuracy: 0.648444, val accuracy: 0.614000\n",
      "Loss: 1.638044, Train accuracy: 0.677444, val accuracy: 0.651000\n",
      "Loss: 1.873557, Train accuracy: 0.654333, val accuracy: 0.638000\n",
      "Loss: 1.490486, Train accuracy: 0.655667, val accuracy: 0.625000\n",
      "Loss: 1.439203, Train accuracy: 0.642889, val accuracy: 0.627000\n",
      "Loss: 1.691726, Train accuracy: 0.621222, val accuracy: 0.582000\n",
      "Loss: 1.415094, Train accuracy: 0.690222, val accuracy: 0.660000\n",
      "Loss: 1.681442, Train accuracy: 0.637778, val accuracy: 0.603000\n",
      "Loss: 1.308475, Train accuracy: 0.655889, val accuracy: 0.651000\n",
      "Loss: 1.373048, Train accuracy: 0.661778, val accuracy: 0.639000\n",
      "Loss: 1.340602, Train accuracy: 0.720000, val accuracy: 0.682000\n",
      "Loss: 1.392165, Train accuracy: 0.659889, val accuracy: 0.626000\n",
      "Loss: 1.501174, Train accuracy: 0.632222, val accuracy: 0.596000\n",
      "Loss: 1.554044, Train accuracy: 0.679556, val accuracy: 0.653000\n",
      "Loss: 1.855037, Train accuracy: 0.666111, val accuracy: 0.634000\n",
      "Loss: 1.432517, Train accuracy: 0.707444, val accuracy: 0.681000\n",
      "learning_rate=0.1, reg_strength=0.001, n_layers=200\n",
      "Loss: 1.796049, Train accuracy: 0.402111, val accuracy: 0.397000\n",
      "Loss: 1.737209, Train accuracy: 0.514222, val accuracy: 0.509000\n",
      "Loss: 1.657896, Train accuracy: 0.620778, val accuracy: 0.606000\n",
      "Loss: 1.730133, Train accuracy: 0.537333, val accuracy: 0.512000\n",
      "Loss: 1.946688, Train accuracy: 0.560778, val accuracy: 0.556000\n",
      "Loss: 1.835957, Train accuracy: 0.617333, val accuracy: 0.614000\n",
      "Loss: 1.489235, Train accuracy: 0.612667, val accuracy: 0.571000\n",
      "Loss: 1.163236, Train accuracy: 0.643444, val accuracy: 0.621000\n",
      "Loss: 1.898159, Train accuracy: 0.647111, val accuracy: 0.606000\n",
      "Loss: 1.505904, Train accuracy: 0.649778, val accuracy: 0.610000\n",
      "Loss: 1.591016, Train accuracy: 0.657889, val accuracy: 0.606000\n",
      "Loss: 1.111486, Train accuracy: 0.608556, val accuracy: 0.589000\n",
      "Loss: 1.776712, Train accuracy: 0.598444, val accuracy: 0.583000\n",
      "Loss: 1.890644, Train accuracy: 0.686778, val accuracy: 0.662000\n",
      "Loss: 1.941679, Train accuracy: 0.669111, val accuracy: 0.624000\n",
      "Loss: 1.232561, Train accuracy: 0.675111, val accuracy: 0.642000\n",
      "Loss: 1.586314, Train accuracy: 0.629889, val accuracy: 0.594000\n",
      "Loss: 1.663763, Train accuracy: 0.601667, val accuracy: 0.583000\n",
      "Loss: 2.088913, Train accuracy: 0.646444, val accuracy: 0.607000\n",
      "Loss: 1.719372, Train accuracy: 0.641778, val accuracy: 0.615000\n",
      "Loss: 1.256799, Train accuracy: 0.633667, val accuracy: 0.614000\n",
      "Loss: 1.982783, Train accuracy: 0.629889, val accuracy: 0.606000\n",
      "Loss: 1.369402, Train accuracy: 0.623667, val accuracy: 0.593000\n",
      "Loss: 1.441233, Train accuracy: 0.694556, val accuracy: 0.659000\n",
      "Loss: 1.535914, Train accuracy: 0.680778, val accuracy: 0.641000\n",
      "learning_rate=0.1, reg_strength=0.001, n_layers=500\n",
      "Loss: 1.763137, Train accuracy: 0.422000, val accuracy: 0.443000\n",
      "Loss: 1.508531, Train accuracy: 0.520222, val accuracy: 0.482000\n",
      "Loss: 1.993996, Train accuracy: 0.576111, val accuracy: 0.554000\n",
      "Loss: 1.725549, Train accuracy: 0.604667, val accuracy: 0.593000\n",
      "Loss: 1.386926, Train accuracy: 0.595667, val accuracy: 0.579000\n",
      "Loss: 1.619679, Train accuracy: 0.646667, val accuracy: 0.594000\n",
      "Loss: 1.938593, Train accuracy: 0.617444, val accuracy: 0.583000\n",
      "Loss: 2.025272, Train accuracy: 0.637667, val accuracy: 0.597000\n",
      "Loss: 1.643074, Train accuracy: 0.578444, val accuracy: 0.561000\n",
      "Loss: 3.379520, Train accuracy: 0.598333, val accuracy: 0.567000\n",
      "Loss: 2.156154, Train accuracy: 0.619889, val accuracy: 0.591000\n",
      "Loss: 1.873915, Train accuracy: 0.561222, val accuracy: 0.557000\n",
      "Loss: 2.731514, Train accuracy: 0.600556, val accuracy: 0.561000\n",
      "Loss: 1.581894, Train accuracy: 0.690889, val accuracy: 0.648000\n",
      "Loss: 1.607219, Train accuracy: 0.691222, val accuracy: 0.640000\n",
      "Loss: 1.515639, Train accuracy: 0.641222, val accuracy: 0.599000\n",
      "Loss: 2.312410, Train accuracy: 0.665111, val accuracy: 0.621000\n",
      "Loss: 1.921419, Train accuracy: 0.645778, val accuracy: 0.619000\n",
      "Loss: 1.609419, Train accuracy: 0.646333, val accuracy: 0.618000\n",
      "Loss: 1.756174, Train accuracy: 0.670444, val accuracy: 0.632000\n",
      "Loss: 1.655853, Train accuracy: 0.640889, val accuracy: 0.603000\n",
      "Loss: 1.658888, Train accuracy: 0.657444, val accuracy: 0.614000\n",
      "Loss: 2.055188, Train accuracy: 0.642778, val accuracy: 0.616000\n",
      "Loss: 2.344968, Train accuracy: 0.643556, val accuracy: 0.605000\n",
      "Loss: 1.706365, Train accuracy: 0.665333, val accuracy: 0.631000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=100\n",
      "Loss: 1.703740, Train accuracy: 0.342222, val accuracy: 0.355000\n",
      "Loss: 1.646117, Train accuracy: 0.522889, val accuracy: 0.499000\n",
      "Loss: 1.443203, Train accuracy: 0.592556, val accuracy: 0.565000\n",
      "Loss: 1.274637, Train accuracy: 0.559556, val accuracy: 0.574000\n",
      "Loss: 1.146168, Train accuracy: 0.620111, val accuracy: 0.576000\n",
      "Loss: 1.381717, Train accuracy: 0.689111, val accuracy: 0.658000\n",
      "Loss: 1.295021, Train accuracy: 0.661111, val accuracy: 0.607000\n",
      "Loss: 1.246318, Train accuracy: 0.651556, val accuracy: 0.599000\n",
      "Loss: 1.647531, Train accuracy: 0.697333, val accuracy: 0.629000\n",
      "Loss: 0.991776, Train accuracy: 0.683111, val accuracy: 0.617000\n",
      "Loss: 1.195990, Train accuracy: 0.724556, val accuracy: 0.662000\n",
      "Loss: 1.693759, Train accuracy: 0.730667, val accuracy: 0.659000\n",
      "Loss: 1.137240, Train accuracy: 0.755889, val accuracy: 0.674000\n",
      "Loss: 1.303599, Train accuracy: 0.753000, val accuracy: 0.680000\n",
      "Loss: 1.069639, Train accuracy: 0.753556, val accuracy: 0.665000\n",
      "Loss: 1.362463, Train accuracy: 0.743889, val accuracy: 0.669000\n",
      "Loss: 1.034071, Train accuracy: 0.754556, val accuracy: 0.679000\n",
      "Loss: 1.025478, Train accuracy: 0.696000, val accuracy: 0.622000\n",
      "Loss: 1.189617, Train accuracy: 0.750667, val accuracy: 0.639000\n",
      "Loss: 0.800714, Train accuracy: 0.760000, val accuracy: 0.695000\n",
      "Loss: 1.299140, Train accuracy: 0.746444, val accuracy: 0.672000\n",
      "Loss: 0.980017, Train accuracy: 0.779111, val accuracy: 0.680000\n",
      "Loss: 1.009139, Train accuracy: 0.784667, val accuracy: 0.669000\n",
      "Loss: 1.642194, Train accuracy: 0.759111, val accuracy: 0.663000\n",
      "Loss: 0.865310, Train accuracy: 0.781222, val accuracy: 0.660000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=200\n",
      "Loss: 1.639092, Train accuracy: 0.378222, val accuracy: 0.362000\n",
      "Loss: 1.484058, Train accuracy: 0.525889, val accuracy: 0.535000\n",
      "Loss: 1.651301, Train accuracy: 0.525444, val accuracy: 0.537000\n",
      "Loss: 1.504297, Train accuracy: 0.606000, val accuracy: 0.577000\n",
      "Loss: 1.195408, Train accuracy: 0.659778, val accuracy: 0.609000\n",
      "Loss: 1.032109, Train accuracy: 0.622556, val accuracy: 0.604000\n",
      "Loss: 1.791559, Train accuracy: 0.669778, val accuracy: 0.606000\n",
      "Loss: 1.048494, Train accuracy: 0.699556, val accuracy: 0.631000\n",
      "Loss: 0.872581, Train accuracy: 0.683889, val accuracy: 0.639000\n",
      "Loss: 1.492348, Train accuracy: 0.713889, val accuracy: 0.652000\n",
      "Loss: 1.210327, Train accuracy: 0.688778, val accuracy: 0.642000\n",
      "Loss: 1.636641, Train accuracy: 0.668111, val accuracy: 0.635000\n",
      "Loss: 0.921954, Train accuracy: 0.733667, val accuracy: 0.668000\n",
      "Loss: 1.211749, Train accuracy: 0.748778, val accuracy: 0.656000\n",
      "Loss: 1.337668, Train accuracy: 0.760667, val accuracy: 0.657000\n",
      "Loss: 0.922466, Train accuracy: 0.727667, val accuracy: 0.629000\n",
      "Loss: 1.235190, Train accuracy: 0.773778, val accuracy: 0.693000\n",
      "Loss: 1.039630, Train accuracy: 0.735000, val accuracy: 0.663000\n",
      "Loss: 1.988090, Train accuracy: 0.746667, val accuracy: 0.649000\n",
      "Loss: 1.624032, Train accuracy: 0.765889, val accuracy: 0.658000\n",
      "Loss: 1.801691, Train accuracy: 0.761222, val accuracy: 0.679000\n",
      "Loss: 1.389157, Train accuracy: 0.760556, val accuracy: 0.667000\n",
      "Loss: 0.942510, Train accuracy: 0.794444, val accuracy: 0.702000\n",
      "Loss: 1.196982, Train accuracy: 0.806667, val accuracy: 0.687000\n",
      "Loss: 1.021183, Train accuracy: 0.779333, val accuracy: 0.662000\n",
      "learning_rate=0.1, reg_strength=0.0001, n_layers=500\n",
      "Loss: 1.759749, Train accuracy: 0.355444, val accuracy: 0.356000\n",
      "Loss: 1.651349, Train accuracy: 0.492889, val accuracy: 0.495000\n",
      "Loss: 1.648081, Train accuracy: 0.624778, val accuracy: 0.618000\n",
      "Loss: 1.315549, Train accuracy: 0.607667, val accuracy: 0.600000\n",
      "Loss: 1.589305, Train accuracy: 0.624000, val accuracy: 0.588000\n",
      "Loss: 2.094079, Train accuracy: 0.641556, val accuracy: 0.614000\n",
      "Loss: 1.622407, Train accuracy: 0.686000, val accuracy: 0.642000\n",
      "Loss: 1.359505, Train accuracy: 0.698111, val accuracy: 0.639000\n",
      "Loss: 1.132626, Train accuracy: 0.709222, val accuracy: 0.630000\n",
      "Loss: 1.529441, Train accuracy: 0.749778, val accuracy: 0.682000\n",
      "Loss: 1.235075, Train accuracy: 0.744222, val accuracy: 0.670000\n",
      "Loss: 1.763379, Train accuracy: 0.705222, val accuracy: 0.627000\n",
      "Loss: 1.909520, Train accuracy: 0.720333, val accuracy: 0.643000\n",
      "Loss: 1.080676, Train accuracy: 0.739111, val accuracy: 0.652000\n",
      "Loss: 2.246156, Train accuracy: 0.721222, val accuracy: 0.624000\n",
      "Loss: 1.003257, Train accuracy: 0.731667, val accuracy: 0.630000\n",
      "Loss: 1.500722, Train accuracy: 0.747556, val accuracy: 0.659000\n",
      "Loss: 1.449720, Train accuracy: 0.730444, val accuracy: 0.661000\n",
      "Loss: 1.962196, Train accuracy: 0.770889, val accuracy: 0.678000\n",
      "Loss: 2.693396, Train accuracy: 0.756111, val accuracy: 0.644000\n",
      "Loss: 1.948794, Train accuracy: 0.787667, val accuracy: 0.676000\n",
      "Loss: 4.278190, Train accuracy: 0.732333, val accuracy: 0.640000\n",
      "Loss: 1.214550, Train accuracy: 0.762667, val accuracy: 0.667000\n",
      "Loss: 1.317875, Train accuracy: 0.769111, val accuracy: 0.644000\n",
      "Loss: 1.341305, Train accuracy: 0.791111, val accuracy: 0.687000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=100\n",
      "Loss: 1.894921, Train accuracy: 0.360889, val accuracy: 0.371000\n",
      "Loss: 1.442207, Train accuracy: 0.518444, val accuracy: 0.522000\n",
      "Loss: 1.563579, Train accuracy: 0.575000, val accuracy: 0.562000\n",
      "Loss: 1.379786, Train accuracy: 0.584556, val accuracy: 0.570000\n",
      "Loss: 1.493261, Train accuracy: 0.673556, val accuracy: 0.630000\n",
      "Loss: 1.032052, Train accuracy: 0.675778, val accuracy: 0.644000\n",
      "Loss: 1.249720, Train accuracy: 0.649556, val accuracy: 0.604000\n",
      "Loss: 1.397782, Train accuracy: 0.706444, val accuracy: 0.664000\n",
      "Loss: 1.954938, Train accuracy: 0.730778, val accuracy: 0.662000\n",
      "Loss: 0.944746, Train accuracy: 0.717444, val accuracy: 0.643000\n",
      "Loss: 0.892320, Train accuracy: 0.739556, val accuracy: 0.673000\n",
      "Loss: 0.779564, Train accuracy: 0.725444, val accuracy: 0.653000\n",
      "Loss: 1.316543, Train accuracy: 0.740222, val accuracy: 0.648000\n",
      "Loss: 0.663029, Train accuracy: 0.778778, val accuracy: 0.681000\n",
      "Loss: 1.383827, Train accuracy: 0.724778, val accuracy: 0.638000\n",
      "Loss: 1.274041, Train accuracy: 0.766222, val accuracy: 0.680000\n",
      "Loss: 1.458647, Train accuracy: 0.757222, val accuracy: 0.670000\n",
      "Loss: 0.648658, Train accuracy: 0.768778, val accuracy: 0.659000\n",
      "Loss: 0.832704, Train accuracy: 0.761778, val accuracy: 0.675000\n",
      "Loss: 1.093319, Train accuracy: 0.807667, val accuracy: 0.695000\n",
      "Loss: 0.844257, Train accuracy: 0.795889, val accuracy: 0.693000\n",
      "Loss: 1.317590, Train accuracy: 0.754111, val accuracy: 0.678000\n",
      "Loss: 0.569164, Train accuracy: 0.802778, val accuracy: 0.703000\n",
      "Loss: 0.673565, Train accuracy: 0.812333, val accuracy: 0.691000\n",
      "Loss: 1.394495, Train accuracy: 0.812111, val accuracy: 0.705000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=200\n",
      "Loss: 1.496014, Train accuracy: 0.391778, val accuracy: 0.384000\n",
      "Loss: 1.250960, Train accuracy: 0.527333, val accuracy: 0.512000\n",
      "Loss: 0.892042, Train accuracy: 0.615889, val accuracy: 0.552000\n",
      "Loss: 1.398581, Train accuracy: 0.668333, val accuracy: 0.646000\n",
      "Loss: 1.788962, Train accuracy: 0.671222, val accuracy: 0.637000\n",
      "Loss: 1.178276, Train accuracy: 0.665556, val accuracy: 0.636000\n",
      "Loss: 1.042599, Train accuracy: 0.688444, val accuracy: 0.624000\n",
      "Loss: 1.372758, Train accuracy: 0.682333, val accuracy: 0.622000\n",
      "Loss: 1.410003, Train accuracy: 0.698000, val accuracy: 0.641000\n",
      "Loss: 1.447903, Train accuracy: 0.757556, val accuracy: 0.689000\n",
      "Loss: 1.522869, Train accuracy: 0.724889, val accuracy: 0.673000\n",
      "Loss: 0.588848, Train accuracy: 0.746444, val accuracy: 0.674000\n",
      "Loss: 2.161713, Train accuracy: 0.755333, val accuracy: 0.667000\n",
      "Loss: 0.860624, Train accuracy: 0.783000, val accuracy: 0.697000\n",
      "Loss: 1.341381, Train accuracy: 0.748778, val accuracy: 0.665000\n",
      "Loss: 1.616526, Train accuracy: 0.778556, val accuracy: 0.696000\n",
      "Loss: 0.713644, Train accuracy: 0.755000, val accuracy: 0.676000\n",
      "Loss: 1.434663, Train accuracy: 0.793111, val accuracy: 0.690000\n",
      "Loss: 0.685747, Train accuracy: 0.790222, val accuracy: 0.687000\n",
      "Loss: 0.539752, Train accuracy: 0.799000, val accuracy: 0.690000\n",
      "Loss: 0.509251, Train accuracy: 0.811222, val accuracy: 0.706000\n",
      "Loss: 1.106927, Train accuracy: 0.761000, val accuracy: 0.668000\n",
      "Loss: 0.795948, Train accuracy: 0.809778, val accuracy: 0.700000\n",
      "Loss: 0.793745, Train accuracy: 0.793444, val accuracy: 0.684000\n",
      "Loss: 0.687045, Train accuracy: 0.792778, val accuracy: 0.678000\n",
      "learning_rate=0.1, reg_strength=1e-05, n_layers=500\n",
      "Loss: 1.504617, Train accuracy: 0.434222, val accuracy: 0.423000\n",
      "Loss: 1.163439, Train accuracy: 0.585889, val accuracy: 0.575000\n",
      "Loss: 1.396092, Train accuracy: 0.603222, val accuracy: 0.612000\n",
      "Loss: 1.344943, Train accuracy: 0.656667, val accuracy: 0.632000\n",
      "Loss: 1.679857, Train accuracy: 0.608778, val accuracy: 0.544000\n",
      "Loss: 1.285668, Train accuracy: 0.669222, val accuracy: 0.616000\n",
      "Loss: 0.846016, Train accuracy: 0.703222, val accuracy: 0.638000\n",
      "Loss: 0.691459, Train accuracy: 0.705333, val accuracy: 0.633000\n",
      "Loss: 0.953357, Train accuracy: 0.720889, val accuracy: 0.651000\n",
      "Loss: 1.912287, Train accuracy: 0.724333, val accuracy: 0.639000\n",
      "Loss: 1.625520, Train accuracy: 0.709444, val accuracy: 0.625000\n",
      "Loss: 1.123248, Train accuracy: 0.730889, val accuracy: 0.653000\n",
      "Loss: 1.083160, Train accuracy: 0.716333, val accuracy: 0.635000\n",
      "Loss: 1.168815, Train accuracy: 0.755222, val accuracy: 0.666000\n",
      "Loss: 0.652766, Train accuracy: 0.740889, val accuracy: 0.651000\n",
      "Loss: 1.005761, Train accuracy: 0.802444, val accuracy: 0.692000\n",
      "Loss: 0.967769, Train accuracy: 0.761778, val accuracy: 0.667000\n",
      "Loss: 0.799935, Train accuracy: 0.786111, val accuracy: 0.676000\n",
      "Loss: 2.213408, Train accuracy: 0.754000, val accuracy: 0.636000\n",
      "Loss: 1.720502, Train accuracy: 0.720111, val accuracy: 0.637000\n",
      "Loss: 0.866348, Train accuracy: 0.771222, val accuracy: 0.658000\n",
      "Loss: 0.700215, Train accuracy: 0.828444, val accuracy: 0.695000\n",
      "Loss: 0.659639, Train accuracy: 0.823000, val accuracy: 0.698000\n",
      "Loss: 0.719847, Train accuracy: 0.793444, val accuracy: 0.683000\n",
      "Loss: 1.572584, Train accuracy: 0.783222, val accuracy: 0.649000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=100\n",
      "Loss: 2.296112, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298590, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.350511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.030163, Train accuracy: 0.254889, val accuracy: 0.248000\n",
      "Loss: 1.887325, Train accuracy: 0.300000, val accuracy: 0.302000\n",
      "Loss: 1.890503, Train accuracy: 0.397333, val accuracy: 0.403000\n",
      "Loss: 1.641257, Train accuracy: 0.459889, val accuracy: 0.447000\n",
      "Loss: 1.404401, Train accuracy: 0.534111, val accuracy: 0.520000\n",
      "Loss: 1.086265, Train accuracy: 0.578444, val accuracy: 0.553000\n",
      "Loss: 1.176840, Train accuracy: 0.620000, val accuracy: 0.613000\n",
      "Loss: 1.217407, Train accuracy: 0.640778, val accuracy: 0.624000\n",
      "Loss: 1.402757, Train accuracy: 0.652667, val accuracy: 0.655000\n",
      "Loss: 1.114377, Train accuracy: 0.675333, val accuracy: 0.668000\n",
      "Loss: 1.177392, Train accuracy: 0.694444, val accuracy: 0.683000\n",
      "Loss: 0.956864, Train accuracy: 0.703111, val accuracy: 0.688000\n",
      "Loss: 1.074716, Train accuracy: 0.716000, val accuracy: 0.702000\n",
      "Loss: 0.924254, Train accuracy: 0.716333, val accuracy: 0.701000\n",
      "Loss: 0.970063, Train accuracy: 0.735889, val accuracy: 0.717000\n",
      "Loss: 1.200070, Train accuracy: 0.748333, val accuracy: 0.715000\n",
      "Loss: 0.831054, Train accuracy: 0.754111, val accuracy: 0.720000\n",
      "Loss: 0.967417, Train accuracy: 0.751000, val accuracy: 0.711000\n",
      "Loss: 0.908148, Train accuracy: 0.770556, val accuracy: 0.731000\n",
      "Loss: 0.881381, Train accuracy: 0.773111, val accuracy: 0.732000\n",
      "Loss: 1.105559, Train accuracy: 0.783222, val accuracy: 0.739000\n",
      "Loss: 1.059448, Train accuracy: 0.791444, val accuracy: 0.723000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=200\n",
      "Loss: 2.199382, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260806, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.170566, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.142500, Train accuracy: 0.268889, val accuracy: 0.266000\n",
      "Loss: 1.940032, Train accuracy: 0.327222, val accuracy: 0.336000\n",
      "Loss: 1.650370, Train accuracy: 0.417889, val accuracy: 0.404000\n",
      "Loss: 1.390690, Train accuracy: 0.501556, val accuracy: 0.486000\n",
      "Loss: 1.393406, Train accuracy: 0.551111, val accuracy: 0.546000\n",
      "Loss: 1.142398, Train accuracy: 0.602778, val accuracy: 0.598000\n",
      "Loss: 1.575792, Train accuracy: 0.653778, val accuracy: 0.634000\n",
      "Loss: 1.212263, Train accuracy: 0.666222, val accuracy: 0.667000\n",
      "Loss: 1.366428, Train accuracy: 0.689333, val accuracy: 0.665000\n",
      "Loss: 0.961738, Train accuracy: 0.699111, val accuracy: 0.677000\n",
      "Loss: 1.141739, Train accuracy: 0.697667, val accuracy: 0.684000\n",
      "Loss: 1.045090, Train accuracy: 0.728333, val accuracy: 0.712000\n",
      "Loss: 1.121881, Train accuracy: 0.724333, val accuracy: 0.709000\n",
      "Loss: 0.885611, Train accuracy: 0.744000, val accuracy: 0.704000\n",
      "Loss: 1.032392, Train accuracy: 0.732778, val accuracy: 0.698000\n",
      "Loss: 0.982502, Train accuracy: 0.751667, val accuracy: 0.714000\n",
      "Loss: 0.806378, Train accuracy: 0.759444, val accuracy: 0.737000\n",
      "Loss: 0.857222, Train accuracy: 0.769667, val accuracy: 0.723000\n",
      "Loss: 1.284395, Train accuracy: 0.766333, val accuracy: 0.713000\n",
      "Loss: 1.042518, Train accuracy: 0.783222, val accuracy: 0.719000\n",
      "Loss: 0.914152, Train accuracy: 0.795000, val accuracy: 0.729000\n",
      "Loss: 0.868868, Train accuracy: 0.796889, val accuracy: 0.734000\n",
      "learning_rate=0.01, reg_strength=0.001, n_layers=500\n",
      "Loss: 2.165979, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.183652, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.116923, Train accuracy: 0.212000, val accuracy: 0.215000\n",
      "Loss: 2.055336, Train accuracy: 0.277111, val accuracy: 0.278000\n",
      "Loss: 1.837909, Train accuracy: 0.383000, val accuracy: 0.381000\n",
      "Loss: 1.484298, Train accuracy: 0.467000, val accuracy: 0.465000\n",
      "Loss: 1.495750, Train accuracy: 0.531333, val accuracy: 0.542000\n",
      "Loss: 1.264157, Train accuracy: 0.583556, val accuracy: 0.576000\n",
      "Loss: 1.297588, Train accuracy: 0.622889, val accuracy: 0.613000\n",
      "Loss: 1.073831, Train accuracy: 0.646333, val accuracy: 0.622000\n",
      "Loss: 1.252585, Train accuracy: 0.679889, val accuracy: 0.648000\n",
      "Loss: 1.116104, Train accuracy: 0.699333, val accuracy: 0.669000\n",
      "Loss: 1.379311, Train accuracy: 0.703000, val accuracy: 0.673000\n",
      "Loss: 1.616160, Train accuracy: 0.712444, val accuracy: 0.689000\n",
      "Loss: 1.234608, Train accuracy: 0.726444, val accuracy: 0.703000\n",
      "Loss: 0.969427, Train accuracy: 0.731333, val accuracy: 0.691000\n",
      "Loss: 1.226471, Train accuracy: 0.762333, val accuracy: 0.726000\n",
      "Loss: 0.706677, Train accuracy: 0.740333, val accuracy: 0.701000\n",
      "Loss: 1.151653, Train accuracy: 0.775778, val accuracy: 0.720000\n",
      "Loss: 1.040472, Train accuracy: 0.773333, val accuracy: 0.710000\n",
      "Loss: 1.009884, Train accuracy: 0.771556, val accuracy: 0.708000\n",
      "Loss: 0.923569, Train accuracy: 0.798000, val accuracy: 0.741000\n",
      "Loss: 0.703166, Train accuracy: 0.805333, val accuracy: 0.735000\n",
      "Loss: 1.074395, Train accuracy: 0.805222, val accuracy: 0.748000\n",
      "Loss: 0.687565, Train accuracy: 0.802667, val accuracy: 0.725000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=100\n",
      "Loss: 2.261520, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223530, Train accuracy: 0.268778, val accuracy: 0.263000\n",
      "Loss: 1.792220, Train accuracy: 0.319222, val accuracy: 0.320000\n",
      "Loss: 1.614852, Train accuracy: 0.409889, val accuracy: 0.406000\n",
      "Loss: 1.635930, Train accuracy: 0.483556, val accuracy: 0.478000\n",
      "Loss: 1.482665, Train accuracy: 0.543222, val accuracy: 0.544000\n",
      "Loss: 1.334319, Train accuracy: 0.588778, val accuracy: 0.585000\n",
      "Loss: 1.018487, Train accuracy: 0.628444, val accuracy: 0.607000\n",
      "Loss: 1.020526, Train accuracy: 0.642333, val accuracy: 0.630000\n",
      "Loss: 0.919418, Train accuracy: 0.652222, val accuracy: 0.635000\n",
      "Loss: 0.977215, Train accuracy: 0.683667, val accuracy: 0.662000\n",
      "Loss: 1.563577, Train accuracy: 0.704889, val accuracy: 0.695000\n",
      "Loss: 0.902629, Train accuracy: 0.723111, val accuracy: 0.696000\n",
      "Loss: 1.013345, Train accuracy: 0.732111, val accuracy: 0.709000\n",
      "Loss: 1.358276, Train accuracy: 0.737333, val accuracy: 0.703000\n",
      "Loss: 1.323929, Train accuracy: 0.748000, val accuracy: 0.717000\n",
      "Loss: 0.825200, Train accuracy: 0.763556, val accuracy: 0.724000\n",
      "Loss: 0.977371, Train accuracy: 0.771667, val accuracy: 0.732000\n",
      "Loss: 0.872948, Train accuracy: 0.777333, val accuracy: 0.724000\n",
      "Loss: 0.655812, Train accuracy: 0.783667, val accuracy: 0.732000\n",
      "Loss: 0.720139, Train accuracy: 0.783444, val accuracy: 0.721000\n",
      "Loss: 0.528066, Train accuracy: 0.804556, val accuracy: 0.736000\n",
      "Loss: 0.719655, Train accuracy: 0.779889, val accuracy: 0.717000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=200\n",
      "Loss: 2.166051, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207132, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.234673, Train accuracy: 0.198778, val accuracy: 0.209000\n",
      "Loss: 2.108016, Train accuracy: 0.269667, val accuracy: 0.268000\n",
      "Loss: 1.939592, Train accuracy: 0.379667, val accuracy: 0.380000\n",
      "Loss: 1.625371, Train accuracy: 0.450333, val accuracy: 0.444000\n",
      "Loss: 1.698888, Train accuracy: 0.522556, val accuracy: 0.517000\n",
      "Loss: 1.039452, Train accuracy: 0.579667, val accuracy: 0.566000\n",
      "Loss: 1.093639, Train accuracy: 0.617778, val accuracy: 0.601000\n",
      "Loss: 1.498038, Train accuracy: 0.655667, val accuracy: 0.640000\n",
      "Loss: 1.261440, Train accuracy: 0.671444, val accuracy: 0.652000\n",
      "Loss: 1.097307, Train accuracy: 0.689333, val accuracy: 0.685000\n",
      "Loss: 0.944336, Train accuracy: 0.704778, val accuracy: 0.687000\n",
      "Loss: 0.848300, Train accuracy: 0.713889, val accuracy: 0.691000\n",
      "Loss: 0.823615, Train accuracy: 0.731444, val accuracy: 0.686000\n",
      "Loss: 0.923852, Train accuracy: 0.745333, val accuracy: 0.703000\n",
      "Loss: 0.908317, Train accuracy: 0.756444, val accuracy: 0.708000\n",
      "Loss: 0.937680, Train accuracy: 0.752444, val accuracy: 0.704000\n",
      "Loss: 1.001618, Train accuracy: 0.767778, val accuracy: 0.721000\n",
      "Loss: 0.744331, Train accuracy: 0.777222, val accuracy: 0.723000\n",
      "Loss: 0.482110, Train accuracy: 0.787667, val accuracy: 0.725000\n",
      "Loss: 0.573438, Train accuracy: 0.800333, val accuracy: 0.724000\n",
      "Loss: 0.596793, Train accuracy: 0.813556, val accuracy: 0.750000\n",
      "Loss: 0.586758, Train accuracy: 0.816667, val accuracy: 0.734000\n",
      "Loss: 0.508817, Train accuracy: 0.814889, val accuracy: 0.735000\n",
      "learning_rate=0.01, reg_strength=0.0001, n_layers=500\n",
      "Loss: 2.195404, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.116270, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.268602, Train accuracy: 0.225667, val accuracy: 0.228000\n",
      "Loss: 1.994429, Train accuracy: 0.283778, val accuracy: 0.289000\n",
      "Loss: 1.832830, Train accuracy: 0.404111, val accuracy: 0.394000\n",
      "Loss: 1.782105, Train accuracy: 0.474778, val accuracy: 0.472000\n",
      "Loss: 1.228186, Train accuracy: 0.552111, val accuracy: 0.533000\n",
      "Loss: 1.430469, Train accuracy: 0.599222, val accuracy: 0.583000\n",
      "Loss: 1.318763, Train accuracy: 0.629333, val accuracy: 0.612000\n",
      "Loss: 1.285530, Train accuracy: 0.674111, val accuracy: 0.658000\n",
      "Loss: 1.257494, Train accuracy: 0.688222, val accuracy: 0.674000\n",
      "Loss: 0.953715, Train accuracy: 0.699333, val accuracy: 0.681000\n",
      "Loss: 0.863727, Train accuracy: 0.724778, val accuracy: 0.696000\n",
      "Loss: 0.978004, Train accuracy: 0.725556, val accuracy: 0.683000\n",
      "Loss: 0.785800, Train accuracy: 0.740556, val accuracy: 0.726000\n",
      "Loss: 0.723961, Train accuracy: 0.732000, val accuracy: 0.696000\n",
      "Loss: 0.996190, Train accuracy: 0.758889, val accuracy: 0.706000\n",
      "Loss: 0.566751, Train accuracy: 0.787556, val accuracy: 0.734000\n",
      "Loss: 0.434315, Train accuracy: 0.786444, val accuracy: 0.727000\n",
      "Loss: 0.702027, Train accuracy: 0.802222, val accuracy: 0.734000\n",
      "Loss: 0.575689, Train accuracy: 0.803222, val accuracy: 0.720000\n",
      "Loss: 0.816902, Train accuracy: 0.823556, val accuracy: 0.746000\n",
      "Loss: 1.022987, Train accuracy: 0.826556, val accuracy: 0.739000\n",
      "Loss: 0.644588, Train accuracy: 0.830778, val accuracy: 0.741000\n",
      "Loss: 1.224413, Train accuracy: 0.842444, val accuracy: 0.744000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=100\n",
      "Loss: 2.189800, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225924, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.141649, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.107552, Train accuracy: 0.261556, val accuracy: 0.259000\n",
      "Loss: 1.904484, Train accuracy: 0.334000, val accuracy: 0.329000\n",
      "Loss: 1.637906, Train accuracy: 0.410000, val accuracy: 0.397000\n",
      "Loss: 1.537432, Train accuracy: 0.489111, val accuracy: 0.496000\n",
      "Loss: 1.500676, Train accuracy: 0.559778, val accuracy: 0.551000\n",
      "Loss: 1.496187, Train accuracy: 0.605000, val accuracy: 0.603000\n",
      "Loss: 1.287287, Train accuracy: 0.645444, val accuracy: 0.640000\n",
      "Loss: 1.199380, Train accuracy: 0.651778, val accuracy: 0.644000\n",
      "Loss: 1.036597, Train accuracy: 0.679889, val accuracy: 0.669000\n",
      "Loss: 0.995056, Train accuracy: 0.681000, val accuracy: 0.654000\n",
      "Loss: 0.951929, Train accuracy: 0.700778, val accuracy: 0.669000\n",
      "Loss: 0.843353, Train accuracy: 0.735333, val accuracy: 0.689000\n",
      "Loss: 1.452719, Train accuracy: 0.740111, val accuracy: 0.687000\n",
      "Loss: 0.634695, Train accuracy: 0.754667, val accuracy: 0.713000\n",
      "Loss: 0.829534, Train accuracy: 0.761444, val accuracy: 0.719000\n",
      "Loss: 0.738169, Train accuracy: 0.774000, val accuracy: 0.733000\n",
      "Loss: 0.752461, Train accuracy: 0.768333, val accuracy: 0.717000\n",
      "Loss: 0.561595, Train accuracy: 0.779889, val accuracy: 0.722000\n",
      "Loss: 0.563144, Train accuracy: 0.789222, val accuracy: 0.731000\n",
      "Loss: 0.508931, Train accuracy: 0.779778, val accuracy: 0.711000\n",
      "Loss: 0.552761, Train accuracy: 0.805222, val accuracy: 0.732000\n",
      "Loss: 0.624530, Train accuracy: 0.816111, val accuracy: 0.737000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=200\n",
      "Loss: 2.224564, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226500, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.242537, Train accuracy: 0.202222, val accuracy: 0.212000\n",
      "Loss: 2.133183, Train accuracy: 0.273222, val accuracy: 0.274000\n",
      "Loss: 1.670415, Train accuracy: 0.369889, val accuracy: 0.367000\n",
      "Loss: 1.603076, Train accuracy: 0.450444, val accuracy: 0.441000\n",
      "Loss: 1.191408, Train accuracy: 0.503556, val accuracy: 0.517000\n",
      "Loss: 1.259247, Train accuracy: 0.571667, val accuracy: 0.574000\n",
      "Loss: 1.213431, Train accuracy: 0.623000, val accuracy: 0.609000\n",
      "Loss: 1.433817, Train accuracy: 0.659889, val accuracy: 0.654000\n",
      "Loss: 1.276216, Train accuracy: 0.675556, val accuracy: 0.662000\n",
      "Loss: 1.034687, Train accuracy: 0.699556, val accuracy: 0.677000\n",
      "Loss: 1.418192, Train accuracy: 0.696778, val accuracy: 0.682000\n",
      "Loss: 0.849418, Train accuracy: 0.719111, val accuracy: 0.692000\n",
      "Loss: 0.749408, Train accuracy: 0.725556, val accuracy: 0.702000\n",
      "Loss: 0.989595, Train accuracy: 0.736222, val accuracy: 0.685000\n",
      "Loss: 0.850932, Train accuracy: 0.742667, val accuracy: 0.710000\n",
      "Loss: 0.875204, Train accuracy: 0.761556, val accuracy: 0.708000\n",
      "Loss: 0.768124, Train accuracy: 0.763778, val accuracy: 0.716000\n",
      "Loss: 0.939055, Train accuracy: 0.774111, val accuracy: 0.711000\n",
      "Loss: 0.888428, Train accuracy: 0.781889, val accuracy: 0.719000\n",
      "Loss: 0.869331, Train accuracy: 0.795444, val accuracy: 0.728000\n",
      "Loss: 0.695023, Train accuracy: 0.793222, val accuracy: 0.713000\n",
      "Loss: 0.647124, Train accuracy: 0.808333, val accuracy: 0.740000\n",
      "Loss: 0.768086, Train accuracy: 0.818222, val accuracy: 0.739000\n",
      "learning_rate=0.01, reg_strength=1e-05, n_layers=500\n",
      "Loss: 2.308410, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.128789, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.129457, Train accuracy: 0.233333, val accuracy: 0.230000\n",
      "Loss: 2.020540, Train accuracy: 0.285222, val accuracy: 0.288000\n",
      "Loss: 1.592583, Train accuracy: 0.400556, val accuracy: 0.398000\n",
      "Loss: 1.567535, Train accuracy: 0.483222, val accuracy: 0.477000\n",
      "Loss: 1.539769, Train accuracy: 0.546778, val accuracy: 0.536000\n",
      "Loss: 1.305637, Train accuracy: 0.582667, val accuracy: 0.572000\n",
      "Loss: 1.508602, Train accuracy: 0.624889, val accuracy: 0.602000\n",
      "Loss: 1.264944, Train accuracy: 0.664889, val accuracy: 0.644000\n",
      "Loss: 0.957485, Train accuracy: 0.683333, val accuracy: 0.674000\n",
      "Loss: 0.996810, Train accuracy: 0.707889, val accuracy: 0.692000\n",
      "Loss: 1.096965, Train accuracy: 0.710667, val accuracy: 0.689000\n",
      "Loss: 0.916619, Train accuracy: 0.733111, val accuracy: 0.702000\n",
      "Loss: 0.889389, Train accuracy: 0.747333, val accuracy: 0.725000\n",
      "Loss: 0.876190, Train accuracy: 0.760333, val accuracy: 0.705000\n",
      "Loss: 0.947994, Train accuracy: 0.767222, val accuracy: 0.726000\n",
      "Loss: 0.697612, Train accuracy: 0.780111, val accuracy: 0.730000\n",
      "Loss: 0.723784, Train accuracy: 0.793111, val accuracy: 0.732000\n",
      "Loss: 0.941551, Train accuracy: 0.803222, val accuracy: 0.734000\n",
      "Loss: 0.884041, Train accuracy: 0.802556, val accuracy: 0.730000\n",
      "Loss: 1.101918, Train accuracy: 0.816333, val accuracy: 0.734000\n",
      "Loss: 0.587256, Train accuracy: 0.823556, val accuracy: 0.736000\n",
      "Loss: 0.824826, Train accuracy: 0.837111, val accuracy: 0.745000\n",
      "Loss: 0.659975, Train accuracy: 0.837000, val accuracy: 0.735000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=100\n",
      "Loss: 2.290707, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281983, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279760, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287247, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224608, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217679, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.216896, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285361, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.184927, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206627, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.129738, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269396, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.139539, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279178, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.232102, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233375, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250923, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154841, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.150052, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.174559, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.122251, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.245243, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235058, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=200\n",
      "Loss: 2.299664, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274998, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.191845, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241876, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288636, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273535, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157978, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221766, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260901, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255436, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271500, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230514, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209468, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237481, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290185, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300312, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161161, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288713, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201913, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.115659, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.060633, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247739, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.001, n_layers=500\n",
      "Loss: 2.290819, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266767, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197677, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.253260, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236960, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.257370, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168564, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.150207, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.318026, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.140689, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.174466, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.189396, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203387, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.055977, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224655, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.190113, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226856, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.162914, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.171981, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228164, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.138106, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.028914, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.230522, Train accuracy: 0.200111, val accuracy: 0.210000\n",
      "Loss: 2.194721, Train accuracy: 0.207778, val accuracy: 0.214000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=100\n",
      "Loss: 2.270248, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.264455, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.274931, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261232, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246049, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.336263, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233814, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211885, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292753, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271410, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.162459, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.307226, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275262, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195526, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239567, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220780, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301857, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.113450, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.180569, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244254, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.194040, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243425, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182050, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.141451, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=200\n",
      "Loss: 2.285562, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.260807, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237885, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261464, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282924, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.281507, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221293, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.172203, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.185812, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.330658, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.174477, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199640, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229878, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.284998, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.158046, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277990, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.125787, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.342432, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262861, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192832, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277391, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.127502, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.200355, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.225485, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.096152, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=0.0001, n_layers=500\n",
      "Loss: 2.275741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255122, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271063, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.240380, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192756, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228313, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.204376, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207384, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.154133, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.102782, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298514, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.165944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.187617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.402100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303633, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177944, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.125378, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.218915, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.126303, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.112349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211145, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 1.972461, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.089237, Train accuracy: 0.197111, val accuracy: 0.206000\n",
      "Loss: 2.139318, Train accuracy: 0.201333, val accuracy: 0.213000\n",
      "Loss: 1.991077, Train accuracy: 0.210333, val accuracy: 0.216000\n",
      "learning_rate=0.001, reg_strength=1e-05, n_layers=100\n",
      "Loss: 2.281238, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265715, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254135, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219836, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256608, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208587, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.256860, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.205038, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261690, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192803, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229461, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.348730, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.181973, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.193586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263093, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.224313, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.142896, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246827, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157043, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.093939, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.177074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.070879, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.200057, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157822, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=1e-05, n_layers=200\n",
      "Loss: 2.283692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271666, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241381, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.263943, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.235557, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.241005, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.247048, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.192764, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.180894, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.176298, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220859, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322192, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275229, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314879, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.151070, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269576, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275482, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.258909, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308674, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208502, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.210359, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229427, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.095512, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.019295, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "learning_rate=0.001, reg_strength=1e-05, n_layers=500\n",
      "Loss: 2.265930, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.261011, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.207933, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203742, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.197168, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203098, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.136241, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.185060, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262171, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203869, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206844, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.221969, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.215503, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289019, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282312, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272550, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226718, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.166831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.198279, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.070234, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.280590, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.079237, Train accuracy: 0.200333, val accuracy: 0.210000\n",
      "Loss: 2.185019, Train accuracy: 0.208444, val accuracy: 0.218000\n",
      "best validation accuracy achieved: 0.744000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = [1e-1, 1e-2, 1e-3]\n",
    "reg_strength = [1e-3, 1e-4, 1e-5]\n",
    "learning_rate_decay = 0.9\n",
    "hidden_layer_size = [100, 200, 500]\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "for lr, reg, n_layers in itertools.product(learning_rates, \n",
    "                                           reg_strength,\n",
    "                                           hidden_layer_size):\n",
    "    print(f'learning_rate={lr}, reg_strength={reg}, n_layers={n_layers}')\n",
    "    \n",
    "    model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=n_layers, reg=reg)\n",
    "    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=lr, num_epochs=num_epochs, batch_size=batch_size)\n",
    "    loss_hist, train_hist, val_hist = trainer.fit()\n",
    "    accuracy = val_hist[-1]\n",
    "    if not best_val_accuracy or accuracy > best_val_accuracy:\n",
    "        best_classifier = model\n",
    "        best_val_accuracy = accuracy\n",
    "        loss_history = loss_hist\n",
    "        train_history = train_hist\n",
    "        val_history = val_hist\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb2946deb80>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAGrCAYAAACBjHUSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABrw0lEQVR4nO3deZxcVZ3//9enqqt635ekl3Q2QiAJIQkNYQdRVsHguIGKu1FHHJ1Rv/qb73dGxxlnHHXcxgURGUVFhhm2gCKIyr4lIXsCSQhZekl6Se97VZ3fH/d2p7rTnU7S3ale3s/Hox637r2nqj7VKYp+9zn3HHPOISIiIiIiIhNfINEFiIiIiIiIyPFRgBMREREREZkkFOBEREREREQmCQU4ERERERGRSUIBTkREREREZJJQgBMREREREZkkFOBEREREREQmCQU4ERGZ8sxsr5m9JdF1iIiIjJYCnIiIiIiIyCShACciItOSmSWb2ffMrNq/fc/Mkv1zBWb2iJk1mdlhM3vGzAL+uS+ZWZWZtZrZa2b25sS+ExERmU6SEl2AiIhIgvxf4HxgGeCAh4D/B/wD8HmgEij0254PODNbCNwKnOucqzazOUDw1JYtIiLTmXrgRERkunof8DXnXK1zrg74J+AW/1wvUAzMds71Oueecc45IAokA4vMLOSc2+ucez0h1YuIyLSkACciItNVCbAvbn+ffwzgW8Bu4HEz22NmXwZwzu0GPgd8Fag1s3vMrAQREZFTRAFORESmq2pgdtx+uX8M51yrc+7zzrl5wA3A3/Vd6+acu9s5d7H/WAf8+6ktW0REpjMFOBERmS5CZpbSdwN+C/w/Mys0swLgH4FfA5jZ9WZ2mpkZ0II3dDJqZgvN7Ap/spMuoNM/JyIickoowImIyHTxe7zA1XdLAdYBm4EtwCvAv/htFwBPAG3AC8CPnXNP4l3/9g2gHjgIFAF/f8regYiITHvmXZMtIiIiIiIiE5164ERERERERCYJBTgREREREZFJQgFORERERERkklCAExERERERmSSSEl3AUAoKCtycOXMSXYaIiIiIiEhCrF+/vt45Vzj4+IQMcHPmzGHdunWJLkNERERERCQhzGzfUMc1hFJERERERGSSUIATERERERGZJBTgREREREREJokRA5yZzTKzv5jZDjPbZmafHaLN+8xss3973szOjju318y2mNlGM9OFbSIiIiIiIifpeCYxiQCfd869YmaZwHoz+6NzbntcmzeAy5xzjWZ2LXA7sDLu/Jucc/VjV7aIiIiIiMj0M2KAc87VADX+/VYz2wGUAtvj2jwf95AXgbIxrjOhmjt7+drD2ynICFOQkUz+oG1eephQUKNRRURERERkfJ3QMgJmNgdYDrx0jGYfBR6N23fA42bmgJ86524f5rlXA6sBysvLT6Sscdfc0csLr9dT39ZDTzQ2ZJuctJAX6tLDFGQmU5AeJj8jeUDQK8jwjqWHg5jZKX4XIiIiIiIy2Zlz7vgammUATwFfd87dP0ybNwE/Bi52zjX4x0qcc9VmVgT8EfiMc+7pY71WRUWFm4jrwDnnaO2OUN/aTUN7Dw1t3dS1edv6tm4a2npoaOuh3t9v6YoM+TwpoQD56cn9Qa8v5OX7Ia8gLvjlpoUJBhT2RERERESmEzNb75yrGHz8uHrgzCwE3Af85hjhbSlwB3BtX3gDcM5V+9taM3sAOA84ZoCbqMyMrJQQWSkh5h21JvrRuiNRDrf3UN/aQ317d3+48wKfd7+muYstVc00tPcQjR0dpgMGeelhCjNTmJ2XxuyCNObmpzM7P505BWnMyEwhoIAnIiIiIjItjBjgzBvr93Ngh3PuO8O0KQfuB25xzu2MO54OBPxr59KBq4CvjUnlk0ByUpDi7FSKs1NHbBuLOZo7e2lo76autYeGuMBX39bDoZYudta28qdXD9EbPRL0UkIBZud5YW5OXLCbk5/OzCyFOxERERGRqeR4euAuAm4BtpjZRv/Y3wPlAM6524B/BPKBH/vXdkX87r4ZwAP+sSTgbufcH8byDUwVgYCRmx4mNz3MaUXDt4vGHNVNnextaGdvQwf76tvZ29DO63Xt/OXVugHX6CUnBZidn8bs/HTmFqQzO98LdnMK0ilWuBMRERERmXSO+xq4U2miXgM30UVjjprmTvY1dHgBr94PeX7Y64kcCXfhpIA3JDM/nTn5acwpSPd78NIoyUnVdXciIiIiIgk0qmvgZHIIBoyy3DTKctO46LSCAediMcfBli4/2PWFOu/+M7vq6I4Pd8EAs/JS+3vr5uSncfasHM4qzdbsmSIiIiIiCaQAN00EAkZJTiolOalcOH/guVjMcai1qz/YvdHQzr56rxfv+dcb6OyNAjC3IJ23nV3CjctLmVuQnoB3ISIiIiIyvWkIpRyTc45DLd08vbOOBzdW8cKeBpyDpWXZrFpWyg1nF1OUmZLoMkVEREREppThhlAqwMkJOdjcxcObqnlwYxXbqlsIGFx0WgGrlpVy9eIZZKaEEl2iiIiIiMikpwAnY253bSsPbazmoY3V7D/cQXJSgLcsmsGqs0u4fGER4aRAoksUEREREZmUFOBk3DjneGV/Ew9trOKRzTUcbu8hOzXEdWcVc+OyEs6dk6clC0REREREToACnJwSvdEYz+6u56ENVTy+/RAdPVFKc1K54ewSVi0r4czirESXKCIiIiIy4SnAySnX0RPhj9sP8eCGKp7eVU805lg4I5NVy0tYtayU0pzURJcoIiIiIjIhKcBJQjW0dfP7LTU8uLGa9fsaAThvTh6rlpdw3ZJictPDCa5QRERERGTiUICTCePA4Q4e2ljFgxur2V3bRihoXHZ6IauWlfKWM2eQGg4mukQRERERkYRSgJMJxznHtuoW1myqZs3Gag62dJEeDnL1kpncuKyUC+fnkxTUTJYiIiIiMv0owMmEFo05XnqjgYc2VPP7rTW0dkUoyEjm+qXF3Li8lLPLsjHTTJYiIiIiMj0owMmk0dUb5cnXanlwQzV/frWWnmiM8+fl8c+rlrBgRmaiyxMRERERGXcKcDIpNXf2cv8rlXzviV20d0f46MVz+Zs3LyA9OSnRpYmIiIiIjJvhApwuMJIJLTs1xIcvmsufP38Zf7WilJ8+vYe3fOcpfr+lhon4xwcRERERkfE0YoAzs1lm9hcz22Fm28zss0O0MTP7gZntNrPNZrYi7tw1Zvaaf+7LY/0GZHrIz0jmm+88m/s+dQE5aWH++jev8IE7X2ZPXVuiSxMREREROWWOpwcuAnzeOXcmcD7waTNbNKjNtcAC/7Ya+AmAmQWBH/nnFwE3D/FYkeN2zuw8Hr71Ir56wyI27m/imu89w7cfe43OnmiiSxMRERERGXcjBjjnXI1z7hX/fiuwAygd1GwVcJfzvAjkmFkxcB6w2zm3xznXA9zjtxU5aUnBAB+6aC5/+sJlXL+0mB/+ZTdv+c5TPL7toIZVioiIiMiUdkLXwJnZHGA58NKgU6XAgbj9Sv/YcMeHeu7VZrbOzNbV1dWdSFkyTRVlpvCd9yzjv1efT3pykNW/Ws9HfrGWfQ3tiS5NRERERGRcHHeAM7MM4D7gc865lsGnh3iIO8bxow86d7tzrsI5V1FYWHi8ZYmwcl4+v/ubS/i/153Jy28c5srvPs33nthJV6+GVYqIiIjI1HJcAc7MQnjh7TfOufuHaFIJzIrbLwOqj3FcZEyFggE+fuk8/vT5y7lq0Qy+98Qurvru0/zl1dpElyYiIiIiMmaOZxZKA34O7HDOfWeYZmuAD/izUZ4PNDvnaoC1wAIzm2tmYeAmv63IuJiZncIP37uC33xsJaGg8eFfrGX1XeuobOxIdGkiIiIiIqM24kLeZnYx8AywBYj5h/8eKAdwzt3mh7wfAtcAHcCHnXPr/MdfB3wPCAJ3Oue+PlJRWshbxkJPJMbPn32DH/xpFw7HZ65YwMcumUtyUjDRpYmIiIiIHNNwC3mPGOASQQFOxlJVUyf/8sh2Ht16kHkF6fzTqsVcskDXWYqIiIjIxDVcgDuhWShFJqPSnFR+8v5z+MWHzyXmHLf8/GU+/ZtXqGnuTHRpIiIiIiInRAFOpo3LFxbxh89dyt9deTpP7DjEm//jKX761Ov0RmMjP1hEREREZAJQgJNpJSUU5G/evIAn/u4yLpyfz789+irXff8ZXni9IdGliYiIiIiMSAFOpqVZeWnc8cFzueMDFXT2Rrn5Zy/y2Xs2UNvSlejSRERERESGpQAn09pbFs3gib+7jL+54jQe3XKQK/7jKX7+7BtENKxSRERERCYgBTiZ9lJCQf7uqoU8/reXcs7sXP75ke1c/5/Psnbv4USXJiIiIiIygAKciG9OQTq/+PC53Pb+FbR09vKu217g8/duor6tO9GliYiIiIgACnAiA5gZ1ywp5onPX8anLp/Pmk1VXPHtJ/n1i/uIxSbemokiIiIiMr0owIkMIS2cxJeuOYNHP3spi0uy+X8PbuWvfvI826tbEl2aiIiIiExjCnAix3BaUQZ3f3wl333P2Rw43MENP3yWf3lkO+3dkUSXJiIiIiLTkAKcyAjMjLcvL+PPn7+c95w7izuefYO3fOcpHtt2EOc0rFJERERETh0FOJHjlJ0W4l/ffhb3fepCslNDfOJX6/n4XeuobOxIdGkiIiIiMk0owImcoHNm5/LwZy7m/153Js/tbuDK7zzNbU+9Tq/WjhMRERGRcaYAJ3ISQsEAH790Hk98/jIuXlDANx59let/8CzrtHaciIiIiIwjBTiRUSjNSeVnH6jg9lvOobWrl3fe9gJfvm8zTR09iS5NRERERKYgBTiRMXDV4pn88e8uY/Wl8/if9ZVc8R9Pcd/6Sk1yIiIiIiJjasQAZ2Z3mlmtmW0d5vwXzWyjf9tqZlEzy/PP7TWzLf65dWNdvMhEkp6cxN9fdyaPfOZi5uSn8fn/2cTNP3uR3bVtiS5NRERERKYIG6mHwMwuBdqAu5xzS0ZoewPwt865K/z9vUCFc67+RIqqqKhw69Yp78nkFYs57ll7gG88uoPO3iifvGw+n37TaaSEgokuTUREREQmATNb75yrGHx8xB4459zTwPHOzHAz8NsTrE1kygkEjPeuLOfPX7icG5aW8J9/3s1V332ap3bWJbo0EREREZnExuwaODNLA64B7os77IDHzWy9ma0e4fGrzWydma2rq9MvuTI1FGQk8533LOPuj60kKWB88M6XufXuV6ht6Up0aSIiIiIyCY3lJCY3AM855+J76y5yzq0ArgU+7Q/HHJJz7nbnXIVzrqKwsHAMyxJJvAtPK+DRz13C3115Oo9vP8Sb/+Mpfvn8XqIxTXIiIiIiIsdvLAPcTQwaPumcq/a3tcADwHlj+Hoik0pyUpC/efMCHv/cpSwrz+Era7bx9h8/x9aq5kSXJiIiIiKTxJgEODPLBi4DHoo7lm5mmX33gauAIWeyFJlO5hSkc9dHzuMHNy+nuqmLt/3wWb66ZhutXb2JLk1EREREJrikkRqY2W+By4ECM6sEvgKEAJxzt/nN3g487pxrj3voDOABM+t7nbudc38Yu9JFJi8z421nl3DZ6YV8+7HX+OULe3l0aw1fuWEx1y6Zif/fjYiIiIjIACMuI5AIWkZAppuNB5r4+/u3sL2mhcsXFvK1ty2hPD8t0WWJiIiISIKc9DICIjL+ls3KYc2tF/GP1y9i7RuHufK7T/Gjv+ymJxJLdGkiIiIiMoEowIlMEEnBAB+5eC5/+vzlXHFGEd967DWu+8EzPL+7nonYUy4iIiIip54CnMgEMzM7hZ+8/xzu/FAFXb1R3nvHS7zjJ8/z+LaDxLTsgIiIiMi0NuIkJiKSGFecMYML5xdw77oD3P70Hlb/aj2nFWWw+tJ53LislHCS/v4iIiIiMt1oEhORSSASjfG7LTXc9tQedtS0MDMrhY9ePJebV5aTkay/w4iIiIhMNcNNYqIAJzKJOOd4elc9tz35Oi/saSArJYlbLpjNhy6cS2FmcqLLExEREZExogAnMsVsOtDET59+nUe3HiQUDPDOc8pYfck85hSkJ7o0ERERERklBTiRKeqN+nZuf3oP971SSSQa49olxXzysvmcVZad6NJERERE5CQpwIlMcbWtXfzXc3v59Yv7aO2KcNFp+XzysvlcfFoBZpbo8kRERETkBCjAiUwTrV293P3Sfn7+7BvUtnazuCSLT1w2n+uWzCQpqJkrRURERCYDBTiRaaY7EuWhDdXc9vTr7KlrpzwvjY9fMpd3VcwiJRRMdHkiIiIicgwKcCLTVCzm+OOOQ9z21Ots2N9EfnqYD104h1sumE1OWjjR5YmIiIjIEBTgRKY55xwvv3GY2556nb+8VkdaOMjN55Xz0YvnUpKTmujyRERERCSOApyI9Hv1YAs/fWoPazZVY8CqZaV84rJ5nD4jM9GliYiIiAgKcCIyhMrGDn7+7Bvc8/IBOnujvPmMIj55+XzOnZOX6NJEREREprXhAtyIU9KZ2Z1mVmtmW4c5f7mZNZvZRv/2j3HnrjGz18xst5l9eXRvQUTGWlluGl+5YTHPf/kK/vYtp/PK/kbeddsLvOMnz/PH7YeIxSbeH3hEREREprMRe+DM7FKgDbjLObdkiPOXA19wzl0/6HgQ2AlcCVQCa4GbnXPbRypKPXAiidHZE+XedQf42TN7qGzs5LSiDD568VxWLSshLZyU6PJEREREpo2T7oFzzj0NHD6J1zwP2O2c2+Oc6wHuAVadxPOIyCmSGg7ywQvn8OQXLuf7Ny0jFAzw/92/hfP/9U987eHtvFHfnugSRURERKa1sfqT+gVmtgmoxuuN2waUAgfi2lQCK4d7AjNbDawGKC8vH6OyRORkJAUDrFpWytvOLmHdvkbuemEfd72wlzufe4NLFhTwgQvmcMUZRQQDluhSRURERKaVsQhwrwCznXNtZnYd8CCwABjqN7thx2s6524HbgdvCOUY1CUio2RmnDsnj3Pn5FF7/Znc8/IBfvPSPj5+1zpKc1J5//mzec+5s8hL13pyIiIiIqfCiEMoR+Kca3HOtfn3fw+EzKwAr8dtVlzTMrweOhGZhIoyU/ibNy/g2S9dwU/et4LyvDT+/Q+vcv6//Ym/u3cjGw80JbpEERERkSlv1D1wZjYTOOScc2Z2Hl4obACagAVmNheoAm4C3jva1xORxAoFA1x7VjHXnlXMrkOt/OrFfdy3vpL7X6liaVk2t5w/mxvOLiElFEx0qSIiIiJTzvHMQvlb4HKgADgEfAUIATjnbjOzW4FPARGgE/g759zz/mOvA74HBIE7nXNfP56iNAulyOTS2tXLAxuquOuFfeyubSMnLcR7Kmbx/vNnMysvLdHliYiIiEw6WshbRMadc44X9jTwqxf28fj2Q8Sc400Li7jlgtlctqCQgCY9ERERETkuCnAickrVNHfy25f2c/fLB6hv62Z2fhrvXzmbd1WUkZOmSU9EREREjkUBTkQSoicS4w/bDvKrF/aydm8jyUkBVi0r4QMXzGFJaXaiyxMRERGZkBTgRCThtle38KsX9/Hghio6e6MsL8/hAxfM5rqziklO0qQnIiIiIn0U4ERkwmju7OW+9ZX8+sV97KlvJz89zHvOncX7zp9NaU5qossTERERSTgFOBGZcGIxx3Ov13PXC/v4045DALz5zBl84ILZXDS/QJOeiIiIyLQ1XIAb9TpwIiInKxAwLllQyCULCqls7ODul/Zzz9oD/HH7IeYVpPP+82dz4/JS8tI16YmIiIgIqAdORCaY7kiU32+p4a4X9rFhfxNJAeOSBQWsWlbKlYtmkJ6svzuJiIjI1KchlCIy6eyoaeGhjdWs2VhFdXMXKaEAVy6ayaqzS7j09ELCSYFElygiIiIyLhTgRGTSisUc6/c38tDGKn63uYbGjl6yU0Ncd9ZM3nZ2KSvn5ul6OREREZlSFOBEZErojcZ4dlc9D22s4vHth+joiTIzK4Ubzi5m1bJSFpdkYaYwJyIiIpObApyITDkdPRGe2FHLmo1VPPlaHZGYY15hOqvOLuVty0qYW5Ce6BJFRERETooCnIhMaY3tPTy69SAPbazipTcOA3B2WTZvW1bKDUuLKcpKSXCFIiIiIsdPAU5Epo3qpk4e2VzNQxur2VbdghlcOD+fVWeXcvWSmWSnhhJdooiIiMgxKcCJyLS0u7aNNZuqeWhjFfsaOggHA7zpjEJWLSvlijOKSAkFE12iiIiIyFEU4ERkWnPOsbmymYc2VvPw5mrqWrvJSE7iqsUzuHFZKRfOzycpqGUJREREZGI46QBnZncC1wO1zrklQ5x/H/Alf7cN+JRzbpN/bi/QCkSByFAFDEUBTkTGUzTmeHFPAw9trOLRrQdp7YpQkBHmrWcV87Zlpawoz9FMliIiIpJQowlwl+IFs7uGCXAXAjucc41mdi3wVefcSv/cXqDCOVd/IsUqwInIqdLVG+XJ1+pYs6mKJ3bU0hOJMSsvlbedXcL1S0s4Y2amwpyIiIiccqMaQmlmc4BHhgpwg9rlAludc6X+/l4U4ERkkmjp6uXxbYd4aGMVz+2uJ+agODuFSxcUctnCQi46rUAToIiIiMgpcaoC3BeAM5xzH/P33wAaAQf81Dl3+zEeuxpYDVBeXn7Ovn37RqxLRGS81LV288SOQzy9s45nd9XT2h0hGDCWz8rh0tMLuez0Qs4qzSYQUO+ciIiIjL1xD3Bm9ibgx8DFzrkG/1iJc67azIqAPwKfcc49PdLrqQdORCaS3miMjQeaeHpnHU/trGNzZTMAeelhLllQwGWnF3LJgkIKM5MTXKmIiIhMFcMFuKQxevKlwB3AtX3hDcA5V+1va83sAeA8YMQAJyIykYSCAc6dk8e5c/L4/FULqW/r5tld9Ty1s45ndtXx0MZqABaXZHGZ3zu3YnYuIc1qKSIiImNs1AHOzMqB+4FbnHM7446nAwHnXKt//yrga6N9PRGRRCvISObG5aXcuLyUWMyxvaaFp3bW8dRrdfz06T38+MnXyUhO4sL5+Vy2sJBLFxQyKy8t0WWLiIjIFHA8s1D+FrgcKAAOAV8BQgDOudvM7A7gHUDfRWsR51yFmc0DHvCPJQF3O+e+fjxFaQiliExWLV29PL+7gad21vH0zjqqmjoBmF+Y3n/t3Pnz8rWAuIiIiByTFvIWETnFnHO8Xtfu9c7trOOlPQ10R2IkJwU4b24el51eyOULC5lfmKGlCkRERGQABTgRkQTr6o3y0huHeeq1Op7aWcvrde0AlOak+r1zBVx4WgFZKVqqQEREZLpTgBMRmWAOHO7g6V3eUMvndjfQ5i9VcE55LpctLGTl3DwWlWSRFh6T+aZERERkElGAExGZwHqjMV7Z1+hdO7erjq1VLQAEDOYXZrCkNJvFJVmcVZrNopIsMtVLJyIiMqUpwImITCJ1rd1sPNDE1qpmtlU3s6WqmUMt3f3n5xWks7g0m7NKs1hSks3ikmyy0xTqREREpopxXQdORETGVmFmMlcumsGVi2b0H6tt7WJbVQtbq7xA98q+Rh7eVN1/vjwvjSWlWSwpzWZJSTZLSrPJSw8nonwREREZJwpwIiKTRFFmCkVnpPCmM4r6jx1u72FrVTNbq5u9bVULv99ysP98aU5q/9DLJf6tMDM5EeWLiIjIGFCAExGZxPLSw1x6eiGXnl7Yf6y5o5dt1V6o21LVwraqZh7ffqj//Iys5P4euiWl2ZxVms2MrGQtZSAiIjIJKMCJiEwx2WkhLjzNW5KgT2tXL9urW9ha3eL31DXzl9dqifmXQRdkhOOGXnrDMEtzUhXqREREJhgFOBGRaSAzJcTKefmsnJfff6yjJ8KOmha2VDb3B7tndtUT9VNdTlqIxSX+JCn+LJhz89MJBBTqREREEkUBTkRkmkoLJ3HO7DzOmZ3Xf6yrN8qrB1vZUtXM9mrvmrr/em4vPdEYAOnhIItKslhc4gW6JaXZnFaUQSgYSNTbEBERmVYU4EREpF9KKMiyWTksm5XTf6wnEmN3bRtbq5vZVtXMtuoW7l13gI6eKADhpABnzMwcEOrOmJlJSiiYoHchIiIydWkdOBEROWHRmOON+na2VXuBbqsf7Jo7ewEIBozTCjNY3L9OXZYWIBcRETkBWshbRETGlXOOysbOo0JdbeuRBcjnFqSzyL+ubkmpNxRTa9WJiIgcTQt5i4jIuDIzZuWlMSsvjWuWFPcfr23pYlt1i7e0QVULmw408bvNNf3nS7JT+idJ6VveQMsaiIiIDE0BTkRExlVRVgpFWQMXIG/q6PGXNTjSW/fEjkP0DQrJSw+zoCiDBTMyWFCUyYKiDE6bkUFhhoKdiIhMbyMGODO7E7geqHXOLRnivAHfB64DOoAPOede8c9d458LAnc4574xhrWLiMgklZMWPmqtuvbuCK8ebGFrVQvbq1vYXdfGQxurae2K9LfJSkliwQw/0BVlsGBGJqcVZVCSnaJgJyIi08KI18CZ2aVAG3DXMAHuOuAzeAFuJfB959xKMwsCO4ErgUpgLXCzc277SEXpGjgREQHvurq61m521bax61Aru2rb2O3fGtp7+tulh4OcVpTBaUWZfq+dF/DKctMIat06ERGZhE76Gjjn3NNmNucYTVbhhTsHvGhmOWZWDMwBdjvn9vgF3OO3HTHAiYiIgHddXd8QzIvieusAGtq62V3bNiDUPbu7jvteqexvk5wUYH5hRlyo83rsZuenae06ERGZlMbiGrhS4EDcfqV/bKjjK4d7EjNbDawGKC8vH4OyRERkKsvPSCY/I5mV8/IHHG/u7PUDXWt/wFu3t5GHNlb3twkFjbkF6SzwA92CGV6P3dyCdJKTtH6diIhMXGMR4IYam+KOcXxIzrnbgdvBG0I5BnWJiMg0lJ0a4pzZuZwzO3fA8fbuCK/XtbHrUBu7/e226mYe3VpDzP+/TjBgzM5LY25BOqW5qZTkpFKak0pprrctzEgmoCGZIiKSQGMR4CqBWXH7ZUA1EB7muIiIyCmXnpzE0rIclpblDDje1RtlT107u+va2H2olZ2H2th3uIO1ew/TEjeBCkA4GKA4J4WS7NT+gFeWc+R+cXYKKSH14ImIyPgZiwC3BrjVv8ZtJdDsnKsxszpggZnNBaqAm4D3jsHriYiIjJmUUJBFJVksKsk66lxrVy/VTV1UNXVQ1dRFVWMnVU2dVDd18uyueg61djF4LrDCzOT+YFeSk+L34KVRkpNCWU4aWalJmjFTRERO2vEsI/Bb4HKgwMwqga8AIQDn3G3A7/FmoNyNt4zAh/1zETO7FXgMbxmBO51z28bhPYiIiIyLzJQQC2eGWDgzc8jzPZEYh1q6qIwLdn0hb0dNC0/sOER3JDbgMRnJSXHBLm6Ypr9flJmimTNFRGRYIy4jkAhaRkBERKYC5xwN7T0Deu4GhL2mTpo6egc8JilglOamUp6Xxqy8NMrjb/lpZKWEEvRuRETkVDrpZQRERETk5JgZBRnJFGQkc/asnCHbtHdHvGAX13tX2djJ/oZ2Ht1SQ+OggJeTFjoq3M3294uzU0jS8ggiIlOaApyIiEgCpScnsWBGJgtmDD1Ms6WrlwOHOzhwuIP9/bdOtlU189jWg0RiR0bSDNV71xfu1HsnIjI1KMCJiIhMYFkpIRaXZLO4JPuoc9GYo6a5k/0DAp63/4etBznc3jOg/eDeu9n+Vr13IiKThwKciIjIJBUMGGW5aZTlpsH8o88P13u3vbqFx7cdpDd6dO/drNw0ijKTKcz0hn4WZIYpzEjxt8nkpoW1Fp6ISAIpwImIiExRI/XeHWzpYl9D+4DeuwOHO3ijvp36tu6jZtAELzTmp4cpyBgc8rz9woxkCvxtdmpIYU9EZIwpwImIiExDwYD1L18wVO+dc47W7gj1rd3Ut/VQ19pNfVv3Udtdh1qpb+uhJ3p02EsKWH/AK8gYGO76toV+D5/WxxMROT4KcCIiInIUMyMrJURWSoh5hcdu65yjpTNCXVywOzrs9fBqTSv1bd0DJl7pEw4GyM8I+8M3U5iRlcyMLG9blJXCDP+YhnCKyHSnACciIiKjYmZkp4XITgtxWlHGMdvGYo7mzt7+YFfXNrCHr7a1m8rGDl7Z33jUJCzg9eoVZfqhrj/kpVCYeSTwzchMISctpB49EZmSFOBERETklAkEjNz0MLnp4WGXTujTHYlS19rNoZZualu6qG3t5lBLl7ff2sUb9e28uOcwzZ29Rz02HAxQlJVMUeaRkFfkh7v4+xq6KSKTjQKciIiITEjJScEjs2weQ1dvX9Dzwt0hP+zVtnRxqLWLXbVtPLu7ntauyBCvETgyVDPT68nLSw+TnxEmPz1Mfoa3X5CerLAnIhOCApyIiIhMaimhILP89eyOpbMnSm3rkZB3KK5Xr7almx01LTyzq5uWIYIeeMM389LDXqDLGCboZYTJS08mPyNMZrICn4iMPQU4ERERmRZSw0Fm56czOz/9mO16IjEaO3qob+vmcHsPDW09NLT30ODv17f10NDezYHGDhraemjrHjrwhYOB/sB3rKDXdzw9HFTgE5ERKcCJiIiIxAn3D6tMOa72Xb1RDrf3+OFumNDX3sPehnYa2nro6IkO+TzJSQGKs1MozU31l3hIozQ3lZKcFMpy0piZnUI4KTCWb1VEJiEFOBEREZFRSAkFKclJpSQn9bjad/ZEaWg/OujVt3VT3dxFVWMnf3mtjrrW7gGPM4MZmSl+qPNDXm4qZf62JCeVjGT9aicy1em/chEREZFTKDUcpCw88uQs3ZEoNU1dVDV1UtXYSWVTJ9X+/U0HmvjD1hp6owPX1MtODfUHu9K4kNe3zU8Pa5imyCR3XAHOzK4Bvg8EgTucc98YdP6LwPvinvNMoNA5d9jM9gKtQBSIOOcqxqh2ERERkSkrOSnInIJ05hQMfc1eLOaoa+umsrGzP+RVNXVQ3dTF/oYOXni94ajr85KTAgNCXXxPXkl2KgWZYdLC+vu+yERmzrljNzALAjuBK4FKYC1ws3Nu+zDtbwD+1jl3hb+/F6hwztUfb1EVFRVu3bp1x9tcRERERAZxztHSGaHSD3VVjR1e0GvqpKrJG6pZ39Z91ONSQ0HyM7yZNgsywuT3TbZy1H6YvLQwSUFdlycyHsxs/VCdX8fzJ5bzgN3OuT3+E90DrAKGDHDAzcBvT7ZQERERERk9MyM7LUR2WjaLS7KHbNPVG/WGZTZ1UtPc5V2T19ZNgz8hS3VTF1uqmmlo6yESO/qP/maQkxqiICMu5Pmzaub7Ya8g48i+llYQGb3jCXClwIG4/Upg5VANzSwNuAa4Ne6wAx43Mwf81Dl3+zCPXQ2sBigvLz+OskRERERkNFJCQeYVZjCvMOOY7fp68+rbu/tDXn3c5CvesR521LTQ0NZDc2fvkM8TTgoME/DC5KaFyU4NkeNvvfshUkLB8XjrIpPW8QS4of5MMty4yxuA55xzh+OOXeScqzazIuCPZvaqc+7po57QC3a3gzeE8jjqEhEREZFT4EhvXoj5hSO3j19Lr8FfN6+hzV9Dz+/ha2jrZtehNurbuumOxIZ9rnBSgJy4QJedGiIrNUROanjAsexUr77s1BA5fpuQhnfKFHQ8Aa4SmBW3XwZUD9P2JgYNn3TOVfvbWjN7AG9I5lEBTkRERESmhhNZS885R1t3hKaOXpo7vVvf/aZOrzevJe5YdVMXO2paae7sHXYR9T7p4SA5aWE/8A0RAvsDX5is1CSyUrzjmSlJCn8yYR1PgFsLLDCzuUAVXkh77+BGZpYNXAa8P+5YOhBwzrX6968CvjYWhYuIiIjI5GdmZKaEyEwJDegxOB690RgtnXHBb1DYiw+FzZ097Klvo6nDa9dzjF4/gLRw0A90R4JdVkqSvx183NvPTDnSRgFQxsuIAc45FzGzW4HH8JYRuNM5t83MPumfv81v+nbgcedce9zDZwAP+BerJgF3O+f+MJZvQERERESmp1Aw4F9Pl3zCj+3qjQ7o8Wvp7KWlq28biduP0NLVS21rF7trI/1thpjTZYDUUPC4w192aoiZ2ckUZ6eSrsXYZQQjLiOQCFpGQEREREQmKuccHT3RAQFvQOAbFP6ObhchOkwCzEpJoiQnleLsFGZmp1KSnUJxjredmZ1CSU6qJnaZJkazjICIiIiIiPjMjPTkJNKTkygeeoWGYxocABs7ejjY3EV1c6e3beqiprmTTZXNHG7vOerxuWkhirO9kFeck0Jxdiol/rbYD3rJSQp5U5UCnIiIiIjIKXQiAbCrN9of7mr8YFfT3EVNcxdVTZ2s29c45LINBRlhirNTvV47vxevOHtgyNN1epOTApyIiIiIyASVEgoypyCdOQXpw7bp6Il4oa7pSC9eTXMn1U1d7Gto58U9DbR2DZyx0wwKM5Ipzk6hMDOFrJQkMlKSyExJ8ieVidsmxx9LIj2cRCCgBdkTRQFORERERGQSSwsnMb8wg/nHWJC9tavX78nroqaprxfP21Y2dtDWHaG1K0Jr18gTtJhBRvLRwe6o4Nd3Sz76eEZyEknqATwpCnAiIiIiIlNc31INC2ZkHrNd3/V5rV0R2rq9CVf6gl1rV4Q2/3788bbuCPVtPbxR3+4fi9ATPfYyDeAt1ZCZ4g0lTQ8nkRYOkpGcRFpyEunhIGnhJNKTj2zTB+2nhf3HJXuPS04K4M9+P6UpwImIiIiICDDw+jwYeSH24XT1Rgf06g3cDjze3hOho8drf6i1i476qHes29uO1CPYJ2D0B7q+bVrY6+1LCwePOucFwiQuPq2Amdkn/15PNQU4EREREREZUymhICmhIAUnsUZfPOccXb2xAYGuoydCW3eUju4I7T1ROnoitHdH/ePx7aK0d0eoa+2mvTsybCi86yPnKcCJiIiIiIiMlpmRGg6SGg7C8Jf4nRDnHN2RmBfquqMUZIbH5olPEQU4ERERERGZNsysv4cwf4xC4amkqV9EREREREQmCQU4ERERERGRSUIBTkREREREZJJQgBMREREREZkkFOBEREREREQmCXPuOFfGO4XMrA7Yl+g6hlAA1Ce6CJl29LmTRNFnTxJBnztJFH32JBGO9bmb7ZwrHHxwQga4icrM1jnnKhJdh0wv+txJouizJ4mgz50kij57kggn87nTEEoREREREZFJQgFORERERERkklCAOzG3J7oAmZb0uZNE0WdPEkGfO0kUffYkEU74c6dr4ERERERERCYJ9cCJiIiIiIhMEgpwIiIiIiIik4QC3HEws2vM7DUz221mX050PTJ9mNleM9tiZhvNbF2i65GpyczuNLNaM9sadyzPzP5oZrv8bW4ia5SpaZjP3lfNrMr/3ttoZtclskaZesxslpn9xcx2mNk2M/usf1zfezKujvHZO6HvPV0DNwIzCwI7gSuBSmAtcLNzbntCC5Npwcz2AhXOOS0sKuPGzC4F2oC7nHNL/GPfBA47577h/+Eq1zn3pUTWKVPPMJ+9rwJtzrlvJ7I2mbrMrBgods69YmaZwHrgRuBD6HtPxtExPnvv5gS+99QDN7LzgN3OuT3OuR7gHmBVgmsSERkzzrmngcODDq8Cfunf/yXe/2BExtQwnz2RceWcq3HOveLfbwV2AKXoe0/G2TE+eydEAW5kpcCBuP1KTuIHLXKSHPC4ma03s9WJLkamlRnOuRrw/ocDFCW4HplebjWzzf4QSw1jk3FjZnOA5cBL6HtPTqFBnz04ge89BbiR2RDHNO5UTpWLnHMrgGuBT/vDjUREprKfAPOBZUAN8B8JrUamLDPLAO4DPueca0l0PTJ9DPHZO6HvPQW4kVUCs+L2y4DqBNUi04xzrtrf1gIP4A3pFTkVDvlj9fvG7NcmuB6ZJpxzh5xzUedcDPgZ+t6TcWBmIbxfoH/jnLvfP6zvPRl3Q332TvR7TwFuZGuBBWY218zCwE3AmgTXJNOAmaX7F7hiZunAVcDWYz9KZMysAT7o3/8g8FACa5FppO8XaN/b0feejDEzM+DnwA7n3HfiTul7T8bVcJ+9E/3e0yyUx8GfyvN7QBC40zn39cRWJNOBmc3D63UDSALu1mdPxoOZ/Ra4HCgADgFfAR4E7gXKgf3Au5xzmmxCxtQwn73L8YYROWAv8Im+65JExoKZXQw8A2wBYv7hv8e7FknfezJujvHZu5kT+N5TgBMREREREZkkNIRSRERERERkklCAExERERERmSQU4ERERERERCYJBTgRETlhZvaomX1w5JZj+ppzzMyZWdJINQxuexKv9fdmdsdo6hURERkPmsRERGSaMLO2uN00oBuI+vufcM79ZhxfO4y3huYc51zbSO2HeY45wBtAyDkXGcO2lwO/ds6VnUxdIiIip9JJ/WVSREQmH+dcRt99M9sLfMw598TgdmaWNFLoOQmXAhtPNrzJ2Binf1sRETmFNIRSRGSaM7PLzazSzL5kZgeB/zKzXDN7xMzqzKzRv18W95gnzexj/v0PmdmzZvZtv+0bZnbtoJe5Dvi9md1kZusGvf7fmtka//5bzWyDmbWY2QEz++ox6o6vIei/fr2Z7QHeOqjth81sh5m1mtkeM/uEfzwdeBQoMbM2/1ZiZl81s1/HPf5tZrbNzJr81z0z7txeM/uCmW02s2Yz+28zSxmm5vlm9mcza/Br/Y2Z5cSdn2Vm9/s/9wYz+2HcuY/HvYftZrbCP+7M7LS4dr8ws38Zxb9tnpn9l5lV++cf9I9vNbMb4tqF/PewbLh/IxERGXsKcCIiAjATyANmA6vx/v/wX/5+OdAJ/HDYR8NK4DW8BZm/CfzczCzu/HXA74A1wEIzWxB37r3A3f79duADQA5eCPuUmd14HPV/HLgeWA5UAO8cdL7WP58FfBj4rpmtcM61A9cC1c65DP9WHf9AMzsd+C3wOaAQ+D3wsD8stM+7gWuAucBS4EPD1GnAvwElwJnALOCr/usEgUeAfcAcoBS4xz/3Lr/dB/z38DagYeQfC3Di/7a/whtiuxgoAr7rH78LeH9cu+uAGufcxuOsQ0RExoACnIiIAMSArzjnup1znc65Bufcfc65DudcK/B14LJjPH6fc+5nzrko8EugGJgBYGbz8K5Fe8051wE8BNzsn1sAnIEX7HDOPemc2+KciznnNuMFp2O9bp93A99zzh1wzh3GC0n9nHO/c8697jxPAY8Dlxznz+Y9wO+cc390zvUC3wZSgQvj2vzAOVftv/bDwLKhnsg5t9t/nm7nXB3wnbj3dx5esPuic67dOdflnHvWP/cx4JvOubX+e9jtnNt3nPUf97+tmRXjBdpPOucanXO9/s8L4NfAdWaW5e/fghf2RETkFFKAExERgDrnXFffjpmlmdlPzWyfmbUATwM5fi/RUA723fFDGkDfNXdvxeu16nM3foDD6317sO8xZrbSzP7iD+9rBj6J16s3khLgQNz+gHBjZtea2YtmdtjMmvB6j47nefueu//5nHMx/7VK49ocjLvfwZH3PoCZFZnZPWZW5f9cfx1Xxyy8IDzUNWqzgNePs97BTuTfdhZw2DnXOPhJ/J7J54B3+MM+rwXGbeIbEREZmgKciIgADJ6S+PPAQmClcy4LbxIS8IYAnqi+4ZN9HgcK/GunbubI8En8+2uAWc65bOC243zNGrzw0ae8746ZJQP34fWczXDO5eAFyr7nHWk65mq84YZ9z2f+a1UdR12D/Zv/ekv9n+v74+o4AJTb0EsfHADmD/OcHXhDHvvMHHT+RP5tDwB58dflDfJLv+Z3AS84507mZyAiIqOgACciIkPJxLs2qsnM8oCvnMyTmFkq3tDAJ/uO+T1M/wt8C+/arD8Oet3DzrkuMzsPr4fueNwL/I2ZlZlZLvDluHNhIBmoAyLmTbByVdz5Q0C+mWUf47nfamZvNrMQXgDqBp4/ztriZQJteD/XUuCLcedexgui3zCzdDNLMbOL/HN3AF8ws3PMc5qZ9YXKjcB7zZvI5RpGHnI67L+tc64Gb1KXH/uTnYTM7NK4xz4IrAA+i3dNnIiInGIKcCIiMpTv4V3nVQ+8CPzhJJ/nzXg9NV2Djt8NvAX4n0FDBv8a+JqZtQL/iBeejsfPgMeATcArwP19J/zrvP7Gf65GvFC4Ju78q3jX2u3xZ5ksiX9i59xreL1O/4n387gBuME513OctcX7J7wA1IzXKxlfZ9R/7tOA/UAl3vV3OOf+B+9atbuBVrwglec/9LP+45qA9/nnjuV7HPvf9hagF3gVb/KXz8XV2InXmzk3vnYRETl1tJC3iIiMGzP7MbDVOffjRNciY8PM/hE43Tn3/hEbi4jImNNC3iIiMp424s3KKFOAP+Tyo3i9dCIikgAaQikiIuPGOXe7f12VTHJm9nG8SU4edc49neh6RESmKw2hFBERERERmSTUAyciIiIiIjJJTMhr4AoKCtycOXMSXYaIiIiIiEhCrF+/vt45Vzj4+IQMcHPmzGHdunWJLkNERERERCQhzGzfUMc1hFJERERERGSSUIATERERERGZJBTgREREREREJgkFOBERERERkUlCAU5ERERERKYd5xwtXb10R6KJLuWETMhZKEVERERERE6Uc47mzl7q27qpa+2hvq37yG3Avne/OxLj1x9dycULChJd+nFTgBMRERERkQkrGnM0dvQcFcLqBu03tPXQ0N5Nb9Qd9RzBgJGXHqYgI5mCjDDzCzMoyPTuz85PS8C7OnkKcCIiIiIiMq6iMUd3JEp3b4zuSIyu3mj/tqmzl/rW7qN6x+pavfuH27uJHZ3JCAXND2TJFGUmc2ZxVn9AK8xM7j9XkBEmNy1MIGCn/o2PAwU4EREREZEpLBpz9EZj9ERjRKL+/UiM3miM3qijJxKjOxKlq9fbdsfv9wetI+f6wteA+3GBrGfwuUh0yF6xoSQnBbzQlZlMWW4qy2bl9IewgrhQVpiRTFZqEmZTI5SdCAU4EREREZFTrDsSZV9DB3vq2jhwuJPuSJQeP1z1+uGqJ+qIROOCVv/9GL2RgfuRAecdvZFY//5QvVcnIylgpISCJCcFvFvffX+blRqiMDPZPx8kJeRtk0MBUvztUeeSAmSnhfpDWkby9AxlJ0IBTkRERERknDS29/B6XZt/a+f1Wu/+/sMdww4LDAUD/bdw0AglDdxPCgYIBY3MUBLhvrZJ3rFw3GNDSUYocOT+gHNBIxz3vPGBamDg8sJWOBggKagJ7CcCBTgRERERkVGIxhyVjR1eSKttHxDYDrf39LcLBwPMLUhnUUkWN5xdwvzCDOYXZlCen0ZqKEgoaOp9khEpwImIiIiIHIf27gh76uIDmhfY3mhopycS62+Xn+7Ncnj14hn9IW1+YQaluakEp8hEGpI4owpwZnYN8H0gCNzhnPvGoPPZwK+Bcv+1vu2c+6/RvKaIiIiIyHhxznGopTsuoPlDH+vaqGnu6m8XMJidn878wnQuX1johbSidOYVZJCbHk7gO5Cp7qQDnJkFgR8BVwKVwFozW+Oc2x7X7NPAdufcDWZWCLxmZr9xzvUM8ZQiIiIiMo0452ho76GqsZO27ggx54g5iDmHc45YjP5jLu6cd56j27tB7WPx58ExcL/vfCQWo7Kxsz+wtfdE+2vMSE5ifmE6F8zLZ35RBvML0/uHPSYnBRP405PpajQ9cOcBu51zewDM7B5gFRAf4ByQad5g3gzgMBAZxWuKiIiIyCQRicY41NpNVWMnVU0d/raTSn9b3dRJV29s5Cc6BUqyU5hflMG7Kmb1h7T5RRkUZSbrujSZUEYT4EqBA3H7lcDKQW1+CKwBqoFM4D3OuSH/KzWz1cBqgPLy8lGUJSIiIiKnQldvlOomL4z1hbOqxk4q/e3Bli6ig6ZazE8PU5qbysIZmVyxsIjS3FRKc1LJTg0RDHiTeAQMAmYEzLC++4G+Y/htjrSzuPZHzh+9Peo54x6vkCaTxWgC3FCf8sGToV4NbASuAOYDfzSzZ5xzLUc90LnbgdsBKioqxmi1ChERERE5WS1dvV4w6wtngwJafVv3gPYBg5lZKZTmpnLunFw/nKX1h7TSnFRSwxp2KDIaowlwlcCsuP0yvJ62eB8GvuGcc8BuM3sDOAN4eRSvKyIiIjIldPRE2FzZzIb9TWytbqY3EiMYGLqXKBCI7zHy7p9I24CZ38M1qK1/vycao7qpq394Y2VjB61dA698CScF+oPYm8840nvWt52ZnUJIa4WJjKvRBLi1wAIzmwtUATcB7x3UZj/wZuAZM5sBLAT2jOI1RURERCYl5xx7GzrYsL+RV/Y3smF/E68ebO0fYliel0ZaODhoYo4jk3YcmdDjyCQd0djxtT1eGclJ/YGsYrbXg1YWF9IK0pMJaBp8kYQ66QDnnIuY2a3AY3jLCNzpnNtmZp/0z98G/DPwCzPbgjfk8kvOufoxqFtERERkQmvt6mXTgWY27G9kw4EmNuxvpLGjF/CC0rJZOfz15fNZUZ7Lslk54zr1/PGEvWDQyExO0rVgIhPcqNaBc879Hvj9oGO3xd2vBq4azWuIiIiITHSxmOP1ujY27G/q713bWduK83u/FhRlcOWiGawoz2V5eS6nFWWc0gWdzYykoIKZyFQwqgAnIiIiMh01dfSw8UATr+z3etY2Hmjqv14sOzXE8vIcrjurmOXlOZw9K4fs1FCCKxaRqUIBTkREROQYojHHawdb2XCgsb+HbU9dO+DNurhwZhY3nF3C8lk5rJidy9z8dF0nJiLjRgFOREREJE59Wzcb44ZCbqpsoqMnCkBeepgV5Tm8Y0UZy8tzWFqWQ0ayfp0SkVNH3zgiIiIybXX0RNhW3cLmymY2VzaxYX8T+w93AJAUMM4szuJd55SxvDyX5eU5lOelaZIPEUkoBTgRERGZFnoiMV472MqmyiY2VzaxubKZnYda+6fZn5mVwtmzsnnfynKWl+dyVmm2Fp0WkQlHAU5ERESmnKg/K+SmA039vWs7alrpicYAyE0LsbQsh6sWzeCsshzOLsumKCslwVWLiIxMAU5EREQmNecc+w939Ae1TZXNbKtqpt2/bi09HGRJaTYfumgOS8uyObssh7LcVA2FlMSKdMOhrVC9EQ7vgVAqJGdCOAOSs7z7yZmQnOFv/WNJyYmufPKLRSHa4996J93PVQFORERE6I5EOdjcRX1bN6mhJLLTQmSnhkgPBydc0DnU0tXfs7apsoktVc00+Qtkh5MCLCrO4p3nlLG0LIezZ2UzryBDs0JKYvV2Qe02L6zVbPS2tdsh5i09QVIqRLvBxUZ+rkAoLtxlxQW8zGECYF8IzBrUJhMCJzBEOBbz6h1wi57gvn/MRQfu9wWpIbdx92O9Qx8/6nHHeI5oD+AGvrf33w+nvfn4fxYJpgAnIiIyxcVijrq2bqqbOqlu6qKmuZOqpk5qmrqobvaO1bd1D/nYpICRleqFuayUpP778bfhjmUmJ406ODV19LCpspktfs/a5somDrV4tQYDxukzMrlm8UyWluWwtCyb02dkEk4KjOo1RUalP6xtOBLYanccCWupuVC8DC78DJQs9+7nlPuP7YDuNuhuhe4Wf9sKPYOPtR05190CbbVeL17fsd6O46s1lO73PoVHDl+JEkyGYMi/hf1baOA24J8PpQ06P7jtUMfDULAgce/vJCjAiYiITGLOOVq6IlQ3dfrBrIuapk4vrDV3Ud3UyaGWLnqjA//inBYOUpKTSnF2CouKsyjOTqUkJ4WCzGQ6e6K0dPbSPMStpStCZWNn/3405oapDMwgM/lIb15/uEsZOvhlpYbo6o2yxe9Z21zZ3D8jJMC8wnQumJff37O2qHgKTDIS6YaDW6ByHRzaAhYcOFxuQO9K1pGek77bifSgnErOee+tuxV6WuPCRl/4GCKcBEOQWQJZJZBVDFmlkFkMKdneh2ki6u2CQ9ug5hhhrWQ5XHgllCw7EtaGez/hdO+WOWN0dUUjcaEv7nbUv4V/i3RDMAkCg2/BI/ctOHB/8PlhjwWHOD7oeYcLVoHgxP23TyAFOBERkQmsq9cb2hgfyAYHtb5rvfokBYwZWSmU5qRyzuxcSnJSKclO8QNbKqU5qWSlJo16aKRzjvZhwl6Lfxt8/FBLd//9nsjww8VKc1JZWpbNzeeVc3ZZNkvKsslKCY2q3oRzzuslqVrvBbaqdV54i/Z459MLwQIn0YMyeAhd5rGHzw01zC6U6v2iHIseo9cnvuenZVCbIW6x3pHrt8CRmqPd0F439HvMKvaCXX/A82+ZftBLL4TAOPe8Dg5r1RuhLj6s5Xkh7XjD2ngKJkFqjneTKUcBTkREBC+MHGzpYmtVC4daunDO4fCGHzog5rw2zkGs75y/f+Q4OJw3Lb1zA/b72w14TjfwuP+Yhrbu/qGO9W09R9VakBGmODuVeYXpXHRaAaU5qRTnpPhBLZXCzGSCJzp0sfUQ7FgD+1+EwoUwayWUVXi9AcMwMzKSk8hITqIkJ/XEXg8vnMYHvubOXgIB46zSbAoyJs+EAsPqOAxVr3hBrXKdF9w6D3vnQmlez8zKT3o/59IKyC498thoxO8taTu+HpT40NW0b2DoOp7hbxbwhqpFOo/vvYXSjr6eKmf28BNvhAf1JPa1CaUNDDiRbmg9CK010FIFLTXQUu3db62Bfc9528HvKZDkhbnM4qEDXpZ/7ngnqujt8icY2eBfs7Zp6LC24Erv37FkGWTPUm+RnBLm3PBDHxKloqLCrVu3LtFliIjIFOWc48DhTrZWN7O1qpmt1S1sq2qmof3osHSyzMCAgJl332zAfsDf7zsXiNvmpoUpzkmlNCfFH9p4pAdtZnYKKaExGjbXUuOFtu0Pwb7nAQcZM6HtkHffgjDzLCg/3wt05ed7vxTL0SI93hDIyvVHAtvh1/2TBoVnQNk5XlArq4DCM71ekvEWP5RxyF6zliMhMdI1qOduqN48P4yditqHE4t5PXWt1X6482+DQ19v+9GPTSsYFPD8bUYRNO49EtZqt3sTbQCk5Xu9aX29agprcoqY2XrnXMVRxxXgRERkKovGHG/Ut7OtL6xVtbCtupmWLu8v6Un+RBhLSrNYUprN4pJsZvlTzMeHKsOwAEeFMPz9wKAgNmE1V3mhbduDcOBF71jRIlh0IyxaBUVnQGcTVK71euMOvOSFkb6emexyKF95JNAVLZq412GNF+e8X/bjh0LWbPaGAAJkzPCDmh/YSpZDSlZCS552nPPC6bECXkvVkR7RPkeFteWQXaawJgmhACciIlNebzTG7to2tlY1s626ha1VzWyvaaHDv0YsnBTgzOIslpR4YW1JSTanz8wgOWmKB5CmA14v2/aHoPJl79iMJUdCW+Hpx358tBcObob9L3mhb/9L0HbQO5ec5fUozTrfC3alFd7QuKmks8kLa/2BbT101HvnklK9X/ZLz4kbCqlf+CeN3k4v2LXVekMt9W8nE8i4BDgzuwb4PhAE7nDOfWPQ+S8C7/N3k4AzgULn3KA/dwykACciIiPpjkR57WArW6ta2FrtLdy842Br/8QYaeEgi0uyWFyS7YW10izmF2YQCk6TKeYb98L2NbD9QS9wAMxc6gW2RTdCwWkn/9zOeddZxQe62u0cGXa55Eigm3X+wGu7JrporzdRRdU6bzhk5Vpo2HXkfMFCP6j5ga1okTdrnojIGBvzAGdmQWAncCVQCawFbnbObR+m/Q3A3zrnrhjpuRXgREQkXkdPhB01rQOGQe481ErEn8I+MyWJJSXZA4ZBzi1IP/GJPCa7w294gW37Q97kC+ANA1t8I5z5NsifP36v3dnk9U4deNEbelm1/shMitmzvCGXs1Z6oW7GklM77LKnwxsq19HgTSzSt+0/1ne8Hup3edeCgTezYfxQyNIV3pT2IiKnwHABbjRXoJ4H7HbO7fFf4B5gFTBkgANuBn47itcTEZEprG9K+r4ZGLdVHxkG+XpdG33LjeWlh1lSms3lCwv7h0HOykud2NedjaeG173Qtu1Bb5gjQMkKuPJrXm9b7pxTU0dqDix4i3cDf9jlFu8auv0verMHbv1f71w4Y+Cwy7JzvckxjkdPhxe4BgSyuCA2IJQ1ettjzayYku1d95SW701oMefSI4EtUVPAi4gcw2h64N4JXOOc+5i/fwuw0jl36xBt0/B66U4bbvikma0GVgOUl5efs2/fvpOqS0REJo6u3iiH23toaOuhob17wLbev993vr6tm+5B64LNzEphSenAYZAzs1LGPqzFot7Mi82V3i3aC5kzj9ySsybWL/L1u7zAtv0hb+ZD8ELQolXeLac8oeUNyTlo2n8k0B14yRuqiPOmsJ+x2At0+fO93ryjApkf1I4ZxnIgLe9IIEvNi9sffDzfW2Q5kbMpiogcw3j0wA31f7Lh0uANwHPHuvbNOXc7cDt4QyhHUZeIiIyTSDRGY0evH7q6qfe3AwPakWOt3UOvPxVOClCQHiY/I5n8jDALijLJzwiT7x8rykzmzOIsCjPHYC0w56Cz0Ztxri+gNVfG7Vd505Efa62sUJo3s2BmMWT626H2U7LHL+jVvupPRPKgf70Z3pDEq/8NzrwBcmaNz+uOFTPIne3dlr7bO9bV7F1jduBlL9RtvPvI1O/xYSyrFGacNTCEDQ5kCmMiMk2M5puuEoj/v0UZUD1M25vQ8EkRkQmrtauXg81d1DR3cbCli4PNXUMEtB4aO3oYauBGwCAvPZmCjDD5GWGWluWQlx7295P7g1lBRpi89DAZyUlj14vW2+mFsJbKI4Gs+cDAgDZ4PahAyFv7KXsWzL7Am3kuq9Tbzy6FYNhfTPigN9tia9zt4BbY9UdvLa3BklIH9txlzBx6/3iCnnNQu+PINW11rwLmTd1/zb97oW0yTQ4ylJRsOO0t3g28xau7mrzwpjAmIjKk0QyhTMKbxOTNQBXeJCbvdc5tG9QuG3gDmOWcG2JFxaNpEhMRkbHhnKOxo5ea5s7+gHaoxQ9qzV3UNHdyqKWbtiF6yrJTQ+RnhClI93rJ8uJCWL5/zAtkyeSkhgiMx4QhsagXmlr8UNYf0CqPBLaOhqMfl17khbJsP5T1TQ/eF9DSiyAwytkou1uh9ZA/Bbm/7Q99ffuHoKf16McmpfihboievLR8b3jh9oegfidgMPsib2jkmTdAVvHo6hYRkUlhzIdQOuciZnYr8BjeMgJ3Oue2mdkn/fO3+U3fDjx+vOFNRESOTzTmqGvt9nvMOvtD2cG4gHawpat/Wv0+AYMZWSnMzE7h9BmZXHp6IcXZKczMTmVmVgrF2SkUZSWPzdposZjXU9XT5gWe7lZvcd3u+P1WL+TE73c2+QvuVoOLDnzOcKYfxsqOLLKbVXYksGWVQtIYDL0cSXKmdxtpOv6+oDegJ68v9B30rgPb/aeBQc8CXmhb+Qk44wYv3ImIiKCFvEVEJqRINEbNgCGNAwPaweYualu7icYGfoeHgwFmZnvhrDg7hZlZcff9gFaQESZppLXQIt3Q1eKFrZ5BYWvwrafND2Xxx9uOBLPjEUw+EoiSM72hdVmlfg/aoIA2Vadx727zQl3bIcg/DTKKEl2RiIgk0HhMYiIiImMoGnO8uKeBhzdV8+jWgzR39g44nx4OUpzjhbCLTiugODuFGVl94SyF4uxUctNCx3dtWU87NB3wZgVs3u9t+/cPeCFiJBbwA1eWNy18cqZ37VL2LEjO8I73BbK+8/3HMgY+Nil8cj+0qSQ5w7uN51ptIiIy6SnAiYgkUCzmeGV/Iw9vquZ3Ww5S39ZNejjI1Ytncv68fIpzjvSiZaaEjv+Ju5qPDmVN+47sdw6aFDgQ8mYxzJ4FC67ytml5g8JXXwDz90NpE2tqfRERkWlAAU5E5BRzzrG1qoWHN1fzyKZqqpu7SE4K8OYzi3jb2SVcvrCIlNAxrj9zzlsPa6ies7773c0DH5OUeiSglSz37ufM9vZzyr2JNEY7qYeIiIiMOwU4EZFTZOehVh7eVM3Dm6rZ29BBKGhcuqCQ/3PNGbxl0QwykuO+krtaoO41P6TFBzR/f/C0+OFMP5SVe9PM55Qf2c8uh/QC9ZaJiIhMAQpwIiLjaG99O49srubhTTW8dqiVgMGF8wv41OXzuXrxTHLS4q796u2CXY/Dlv+BnY9BtPvIuZQcL4zlnwbzr/B7zvoC2ixvEWMFNBERkSlPAU5EZIxVN3Xyu801PLy5ms2V3lDGc+fk8rVVi7l2STGFmXFT3MeisPcZL7Rtf9gb+pheCBUfhnmXHwloKVmJeTMiIiIyoSjAiYiMgbrWbh7dWsPDm6pZu7cRgKVl2fzf687krUuLKclJPdLYOajZCJv/B7be560PFs70Fmk+650w9zII6utZREREjqbfEERETlJzRy9/2FbDw5tqeP71emIOFs7I5AtXnc71S0uYU5A+8AENr8OW//V62xp2eTM/nn61F9pOvwZCqUO/kIiIiIhPAU5E5AS0dUd4YvshHt5UzdO76uiNOubkp/HpN53G9UtLWDgzc+ADWg/Btvth871Q/QpgMOdiuPAzsOht3rVrIiIiIsdJAU5EZARdvVGefK2WNZuq+dOOWrojMUqyU/jwRXO5YWkJS0qzBi6e3dUMOx7xetreeApcDGYuhSv/GZa8A7JLE/dmREREZFJTgBMRGUJXb5QXXm/g4U3VPL79EG3dEQoywtx07ixuOLuEFeW5BAJxoS3SfWQGydf+4M0gmTsHLvk8nPUuKFyYsPciIiIiU4cCnIgIUN/Wzbq9jazfd5h1+xrZWtVMb9SRnRri+qXF3HB2CSvn5pEUjFvsOhaFfc95wyO3rzkyg+Q5H/JCW1mFpvYXERGRMaUAJyLTjnOO1+vaWLe3kXX7Glm/r5E36r2FscPBAEvLsvnIxXM5f14+F80vIJwUiH8w1Gzyetq23getNRDOiJtB8nLNICkiIiLjRr9liMiU19UbZUtVc38P2/p9jTR29AKQmxbinNl53HTuLCrm5LKkNJvkpODRT3J4jzeD5OZ7j8wgueBKOOtfvRkkw2mn+F2JiIjIdKQAJyJTTkNbN+v9nrV1+xrZUtlMTzQGwLyCdK5cNIOK2XmcMyeXeQXpAycgiddWC1vv93rbqtZ5x2ZfDBd8GhatgrS8U/SORERERDwKcCIyqTnn2FPfzvq9jazbd5h1exvZEzcc8qyybD580RzOmZ3LObNzyc9IHu6JoGkfHFgLlS/DgZfh4GZ/Bsmz4Mqv+TNIlp3CdyciIiIy0KgCnJldA3wfCAJ3OOe+MUSby4HvASGg3jl32WheU0Smt+5IlK3+cMi1ext5ZX8jh9t7gL7hkLm8q8IbDnlWaTYpoSGGQwL0dkL1Bi+oVa71bm2HvHOhNCg9By79Iiz+Kyg64xS9OxEREZFjO+kAZ2ZB4EfAlUAlsNbM1jjntse1yQF+DFzjnNtvZkWjrFdEppnG9h7W72tk7b7DrN/byOaqZnoi3nDIuQXpXHFGERWzc6mYk8f8wmGGQzoHTfu9kHbgZa+H7eAWiEW887lzYd7lUHYuzDoPihZrIhIRERGZkEbzG8p5wG7n3B4AM7sHWAVsj2vzXuB+59x+AOdc7SheT0Smib317TywoYrfb6lhV20bAKGgsaQ0mw9eMJuKOXmcMzuXguGGQ/Z2QvXGI0MhK9dB20HvXCgNSlbAhZ+BsvO80JZReGremIiIiMgojSbAlQIH4vYrgZWD2pwOhMzsSSAT+L5z7q6hnszMVgOrAcrLy0dRlohMRofbe/jd5mru31DFhv1NmMH5c/N5+4pSKmbnsbRsmOGQzkFzpR/W1sKBl/zeNW+WSXLnwNxLvZ61snNhxmIIhk7pexMREREZK6MJcENN2+aGeP5zgDcDqcALZvaic27nUQ907nbgdoCKiorBzyMiU1BXb5Q/v1rL/a9U8eRrtURijoUzMvnytWewalkJxdmpRz+ot8tbh62/d22ttxYbQFIqlK7wZonsC2wZGrktIiIiU8doAlwlMCtuvwyoHqJNvXOuHWg3s6eBs4GjApyITA+xmGPt3sM8sKGK322pobUrQlFmMh+5eC43LitlUUnWwAc0Vx4Jagde9sJbX+9azmyYc7E3FHLWuTBjiXrXREREZEobTYBbCywws7lAFXAT3jVv8R4CfmhmSUAYb4jld0fxmiIySe2ubeWBDVU8uKGaqqZO0sJBrlkyk7cvL+XC+QUEA3Gd+vW7YcOvYOt90OyP1E5K8a5du+Cvj1y7ljkjMW9GREREJEFOOsA55yJmdivwGN4yAnc657aZ2Sf987c553aY2R+AzUAMb6mBrWNRuIhMfHWt3Ty8qZoHNlSxpaqZgMElCwr54tULuWrxDNLCcV9B3W2w/UHY8GvY/wJYEBZc6U82cq63Fpt610RERGSaM+cm3uVmFRUVbt26dYkuQ0ROQmdPlMe3H+SBDVU8s6ueaMyxpDSLty8v44aziynKTDnS2DlvWOSGX8G2B6CnDfIXwIpbYOlN6mETERGRacvM1jvnKgYf10JHIjJq0ZjjxT0N3P9KFX/YWkN7T5SS7BQ+cek83r68lAUzMgc+oPUQbL7H622r3wmhdFjydlj+AW/ykaHWchMRERERBTgROXmvHmzhgVeqeGhjNQdbushMTuL6pSXcuLyUlXPzCMRf1xbthV1/9Hrbdj4GLgrlF8CqH8GiGyE5I2HvQ0RERGSyUIATkRNysLmLNZuquP+VKl492EpSwLh8YSH/cP0i3nxm0dFrtdXt9ELbpnugvRYyZnjXtS1/PxQsSMybEBEREZmkFOBEZERt3REe2+pd1/bc6/U4B8tm5fC1VYt561nF5GckD3xAd6t3TduGX3sLaweS4PRrYPktcNpbIKivHhEREZGTod+iRGRYL7zewD1r9/P4tkN09kYpz0vjM1cs4MZlJcwrHDTk0TnY/6IX2rY9AL3tULAQrvoXWPoeLagtIiIiMgYU4ETkKAcOd/DPj2zn8e2HyE4N8VcrSnn78lLOmZ2LDZ5gpPUgbPqtF9wadkM4E856p9fbVlahCUlERERExpACnIj06+qNcttTr/OTJ18nYMb/uWYhH7lo7tHXtUV7vYlINvzKm5jERWH2RXDJ52HRKginJ+YNiIiIiExxCnAignOOP24/xNce2U5lYyfXLy3m/771TIqzUwc2rH3VC22b/xva6yBjJlz0WW9Ckvz5iSleREREZBpRgBOZ5vbUtfFPD2/nqZ11LCjK4O6Pr+TC+QVHGnS1wLb7vSGSlWu9CUkWXuut2Tb/Ck1IIiIiInIK6TcvkWmqvTvCD/+ymzue2UNKUpB/uH4RH7hgNqFgwGtQ9xo8/wPYej/0dkDhmXD1v3oTkqQXHPvJRURERGRcKMCJTDPOOR7ZXMPXf7eDgy1dvGNFGV+6diFFmSlegwNr4bnvwauPQCgNlr7b620rXaEJSUREREQSTAFOZBp57WArX1mzlRf3HGZRcRY/fO9yKubkeUsA7HoCnv0u7HsWUnPhsi/DeashPT/RZYuIiIiITwFOZBpo6erl+0/s4hfP7yUjOYl/vnEJ7z2vnKCLwpb/hWe/B4e2QFYpXP1vsOIDkJwx4vOKiIiIyKmlACcyhcVijgc2VPFvj75KQ3s3N51bzhevXkheOAbr7/SucWvcCwWnw6ofw1nvgqRwossWERERkWEowIlMUVurmvnKmm2s39fIslk53PmhCpYWGKz9Ibz4E2ivhdIKuOrrsPA6CAQSXbKIiIiIjGBUAc7MrgG+DwSBO5xz3xh0/nLgIeAN/9D9zrmvjeY1ReTYmjp6+Pbjr/Gbl/aTlxbmm+9cyjsXJBF4+buw7k7oboHT3gIX/623+LYmJhERERGZNE46wJlZEPgRcCVQCaw1szXOue2Dmj7jnLt+FDWKyHGIxhz/vfYA33rsVZo7e/ngBXP4fEUSmev/Ax69G2K9sPjt3sLbxWcnulwREREROQmj6YE7D9jtnNsDYGb3AKuAwQFORMbZK/sb+cpD29hS1cx5c/P49wtizH3tW/CzhyAQguXvgws/A3nzEl2qiIiIiIzCaAJcKXAgbr8SWDlEuwvMbBNQDXzBObdtqCczs9XAaoDy8vJRlCUyfdS3dfPvj77K/6yvZEZmmN+8uZsLD34Tu//PkJzl9bat/BRkzkh0qSIiIiIyBkYT4Ia6cMYN2n8FmO2cazOz64AHgQVDPZlz7nbgdoCKiorBzyMicSLRGL96cR/f+eNOunp6+Y+zDnBj270En3sF0ovgLV+Fio9ASnaiSxURERGRMTSaAFcJzIrbL8PrZevnnGuJu/97M/uxmRU45+pH8boi09qLexr46pptvH6wkS8Wb+JD7kHCu16H3Dlw/Xfh7PdCKCXRZYqIiIjIOBhNgFsLLDCzuUAVcBPw3vgGZjYTOOScc2Z2HhAAGkbxmiLT1sHmLv719zt4YtMePpnxDKtzf09K4yGYuRTeeSecuQqCWhlEREREZCo76d/2nHMRM7sVeAxvGYE7nXPbzOyT/vnbgHcCnzKzCNAJ3OSc0/BIkRPQE4lx53NvcNef1vNe/sA3M/5ISqQFyi6Bi38C86/QUgAiIiIi04RNxDxVUVHh1q1bl+gyRBLKOceTO+u4fc2TXNX8v7w39CTJrhvOuN5bw62sItElioiIiMg4MbP1zrmjfuHTeCuRCWhzZRP/+cjLXFZ5G79O+gsWChA4+ya46G+gcGGiyxMRERGRBFGAE5lA9jW0860/bCdz+2/5Vui/yQp14s75KIFLPgfZZYkuT0REREQSTAFOZAKob+vmP/+0i80v/4WvJd3JWaE9RGZdQOD678CMRYkuT0REREQmCAU4kQTq6IlwxzNvcO/TG7k1djdfCf0Fl14EV99B0lnv1OQkIiIiIjKAApxIAkSiMf573QF+8MdXeXPnY/wh+V7SAx3Y+Z+Gy74EKVmJLlFEREREJiAFOJFTyDnHY9sO8c3HXiWzfhN3p/+K+aFdMOtiuO5bGi4pIiIiIsekACdyiqzbe5h/e/RV9uzbx79k3sd1yU9Aygy4+uew5B0aLikiIiIiI1KAExlnu2tb+fc/vMafttewOu1p7s68h3C0A7vwVm+4ZHJmoksUERERkUlCAU5knBxq6eJ7T+zkv9ceYGX4DV4s+BVFba9C+SVw3beh6IxElygiIiIik4wCnMgYa+3q5adP7eGOZ/eQHWvmf0seYUXDw2DF8A4NlxQRERGRk6cAJzJGeiIxfvPSPv7zz7tpau/iG+XreEfzLwg2tsGFfwOX/R8NlxQRERGRUVGAExmlWMzx8OZqvv34axw43MkHZ9Xy5Zw7SK3dCnMvhWu/peGSIiIiIjImFOBERuG53fV849FX2VLVzPkzHPcueoDiPf8LmcXwzjth8V9puKSIiIiIjBkFOJGTsL26hW/84VWe3lnHrOwwD567jbN3/RDb267hkiIiIiIybhTgRE5AZWMH33l8Jw9srCIrJcT3L+rhhspvENiyGeZe5i3GXbgw0WWKiIiIyBQ1qgBnZtcA3weCwB3OuW8M0+5c4EXgPc65/x3Na4okQlNHDz/6y25++fw+MPjcBTl8qvfXhNffDZkl8K5fwKIbNVxSRERERMbVSQc4MwsCPwKuBCqBtWa2xjm3fYh2/w48NppCRRKhNxrj58++wY//spvW7gjvWj6Tvy96gZwXvwk9HXDR5+DSL0JyRqJLFREREZFpYDQ9cOcBu51zewDM7B5gFbB9ULvPAPcB547itUROuc6eKJ/6zXqefK2ONy0s5KvLWpn94mdh+xaYd7k3u2Th6YkuU0RERESmkdEEuFLgQNx+JbAyvoGZlQJvB65ghABnZquB1QDl5eWjKEtk9Jo7evnIL9eyYX8j37mumL9q+Bk8dDdklcK7fgmLVmm4pIiIiIiccqMJcEP99uoG7X8P+JJzLmoj/LLrnLsduB2goqJi8POInDK1LV184M6X2VPXzv9eVseK5z4FvR1w8d/CJV/QcEkRERERSZjRBLhKYFbcfhlQPahNBXCPH94KgOvMLOKce3AUrysybvY3dPD+n79EQ1snTyx7hvIXfwylFfD226BgQaLLExEREZFpbjQBbi2wwMzmAlXATcB74xs45+b23TezXwCPKLzJRLWjpoUP3PkyyZFWnp99F9nb/gzLb4G3/gckJSe6PBERERGRkw9wzrmImd2KN7tkELjTObfNzD7pn79tjGoUGXfr9h7mI79Yy5mhg/wq+/uEq/bBdd+Gcz+ma91EREREZMIY1TpwzrnfA78fdGzI4Oac+9BoXktkvPzltVo+9ev1vD19K1+PfZ9ATzJ8YA3MuSjRpYmIiIiIDDCqACcy2T20sYov3LuBf8h+lFs6f4MVL4X3/AZyZo38YBERERGRU0wBTqatX72wl2+sWc+vs3/Oys7n4Kx3w9t+AKHURJcmIiIiIjIkBTiZdpxz/OBPu7nvT8/wWMYPKO3eB1d9HS74tK53ExEREZEJTQFOppVYzPG1R7az+8WH+UPqD0kNBrGb7of5b0p0aSIiIiIiI1KAk2mjNxrj//zPJgq23M5d4XuwgjOwm+6GvLkjP1hEREREZAJQgJNpoas3yt/+6nmufuNfuTH0PG7RKmzVjyE5I9GliYiIiIgcNwU4mfKaO3v58p2/59OH/pHFwX1wxT9gl3xe17uJiIiIyKSjACdTWl1rN9/66c/5l9Z/Iyscw97933D61YkuS0RERETkpCjAyZR1oKGd+376Vb7e/XN6s2cT+sC9ULAg0WWJiIiIiJw0BTiZknZW1fPqzz/O52J/pqn8LeS8705IyU50WSIiIiIio6IAJ1POlldfxd3zft7GLurP+SwFb/0qBAKJLktEREREZNQU4GRK2fDcY5Q9vpp066L+ujsoOO9diS5JRERERGTMKMDJlLFpzQ9YvP6faAgW0HnLgxTMXZ7okkRERERExpQCnEx+kR523nUrZ+//bzYmr2DeJ/+brLyiRFclIiIiIjLmFOBkUnNttdT87N2c3ryBR7PezeV//UNSU5ITXZaIiIiIyLhQgJNJK1a5gda73kNe92HuKvl/3PyxzxMKarISEREREZm6RvXbrpldY2avmdluM/vyEOdXmdlmM9toZuvM7OLRvJ5In+jGe4j+/Cpau6P88szbef/Hv6DwJiIiIiJT3kn3wJlZEPgRcCVQCaw1szXOue1xzf4ErHHOOTNbCtwLnDGagmWai0aIPP4PJL30Y9bGzmTzBT9g9TXnYmaJrkxEREREZNyNZgjlecBu59weADO7B1gF9Ac451xbXPt0wI3i9WS66zhM5N4PkbT3KX4RuZrgtV9n9UULEl2ViIiIiMgpM5oxZ6XAgbj9Sv/YAGb2djN7Ffgd8JHhnszMVvvDLNfV1dWNoiyZkg5tI/rTy3B7n+NLkU+Q+87vcovCm4iIiIhMM6MJcEONWTuqh80594Bz7gzgRuCfh3sy59ztzrkK51xFYWHhKMqSKcU52HofsTveQmNLG++LfoVrbvkCq5Yd9bcCEREREZEpbzRDKCuBWXH7ZUD1cI2dc0+b2XwzK3DO1Y/idWU6iEVhx8Pw7HehZiPbbCGfdX/HNz96FRVz8hJdnYiIiIhIQowmwK0FFpjZXKAKuAl4b3wDMzsNeN2fxGQFEAYaRvGaMtVFumHTPfDc9+Hw6zQkz+IHbjWPJ13BnR+9iDOLsxJdoYiIiIhIwpx0gHPORczsVuAxIAjc6ZzbZmaf9M/fBrwD+ICZ9QKdwHucc5rIRI7W1QLrfwEv/AjaDrI/+XT+vfdzPN5zLtecVcq9Vy9kVl5aoqsUEREREUkom4h5qqKiwq1bty7RZcip0FYHL/0Et/YOrKuZzeFl/HvbtWwOLeOm88r50EVzKc1JTXSVIiIiIiKnlJmtd85VDD4+miGUIievcS88/5+4Db+GSDdPBc/nO93X0ZCyhI9cN5fbKsrITAklukoRERERkQlFAU5OrYNb4bnv4bbeTwzjIXcpP+y+juxZi/jkjfO4atEMkoKjmRxVRERERGTqUoCT8ecc7H/Bm1Fy1+N0BVL5deQafh65luVLFvGti+dxzuzcRFcpIiIiIjLhKcDJ+InFYNdjuGe+i1W+REsgm5/2vpv7g1dzzXlncu9FczUxiYiIiIjICVCAk7EX7fUW337muwTqX+WQFfGj3g/xTPrVvPfyhfzh3HKyU3V9m4iIiIjIiVKAk7HT0wEbfkX02R8QbK3kdWbxw56/Zt/Mq/nwpQv4x7OKCen6NhERERGRk6YAJ6PXcRhe/hmRF28jqeswG2IL+XH0iySdfjUfu3Q+587JxcwSXaWIiIiIyKSnACcnr7kK98IPia37BcFIB09Gl/NffJb5FW/hHy6ay9yC9ERXKCIiIiIypSjAyYmr20nkme9iW+4FF+Oh6IX8b/JfcfHll/Gj88rJSQsnukIRERERkSlJAU6OX+V6up/6NuFdjxIhxD2RK3gy7z287fLz+cXSEsJJur5NRERERGQ8KcDJ8Ho6oP41qN1Bx8u/Iq36ebpcOrdFb2TP3PfxnsuX88F5+bq+TURERETkFFGAE+jtgoZdUPsq1G6HuleJHNxOsHkfhgOgxeXy/dgtdJ99C++/dDGnFWUkuGgRERERkelHAW46ifRAw26o2+GFtbodULsDd3gP5mIARAmyz0rYHilhV+wc3gjMwmYs4vQzl/KJ8+eTl67r20REREREEkUBbiqKRuDwnv6ARu0OqHvVC2+xCACOAIeTy9hFGesiS3k1WspOV0Z31lyWziliRXkOV5TncmZxlq5tExERERGZIBTgJrNYFBr3+gHN71Wr3eENh4z2AOAwejJncTB5Hq/mrOCFlkJeap/JHleMi6SwtDSbFWfncn15DsvLc5mRlZLY9yQiIiIiIsMaVYAzs2uA7wNB4A7n3DcGnX8f8CV/tw34lHNu02heMyFiUW+x6kTqaYO61/qvUaN2B9TvhEjXkTbZ5XTnnU5Vzvls7S3hmeZCHqvNpqUuBEBJdgrL5+XyzvJcVpTnsLgkW71rIiIiIiKTyEkHODMLAj8CrgQqgbVmtsY5tz2u2RvAZc65RjO7FrgdWDmaghOi+QB8/+xEV3FEZgkUnUF09iVUhWezqauYJxvzeLGyh6odnQCEgwGWlGbx7vNzWTE7lxXluczMVu+aiIiIiMhkNpoeuPOA3c65PQBmdg+wCugPcM655+PavwiUjeL1Eic1D677dmJrSErmcNpcXuko4uWDMV7Z18iW15rpjniTjxRnd7OiPJcPXzSHFbNzWVySRXJSMLE1i4iIiIjImBpNgCsFDsTtV3Ls3rWPAo8Od9LMVgOrAcrLy0dR1tg72B3mwy+ckdAaWjp7qWpqAVoIBwMsLs3i/efPZkV5Litm51CcnZrQ+kREREREZPyNJsANtXqzG7Kh2ZvwAtzFwz2Zc+52vCGWVFRUDPk8iRIMGGW5iQ1IqUUZfPiiOSwv93rXUkLqXRMRERERmW5GE+AqgVlx+2VA9eBGZrYUuAO41jnXMIrXS5jCzGR+9oGKRJchIiIiIiLT3GimIFwLLDCzuWYWBm4C1sQ3MLNy4H7gFufczlG8loiIiIiIyLR30j1wzrmImd0KPIa3jMCdzrltZvZJ//xtwD8C+cCPzQwg4pxTV5aIiIiIiMhJMOcm1OVmgHcN3Lp16xJdhoiIiIiISEKY2fqhOr+0irOIiIiIiMgkoQAnIiIiIiIySSjAiYiIiIiITBIT8ho4M6sD9iW6jiEUAPWJLkKmHX3uJFH02ZNE0OdOEkWfPUmEY33uZjvnCgcfnJABbqIys3WaRVNONX3uJFH02ZNE0OdOEkWfPUmEk/ncaQiliIiIiIjIJKEAJyIiIiIiMkkowJ2Y2xNdgExL+txJouizJ4mgz50kij57kggn/LnTNXAiIiIiIiKThHrgREREREREJgkFOBERERERkUlCAe44mNk1Zvaame02sy8nuh6ZPsxsr5ltMbONZrYu0fXI1GRmd5pZrZltjTuWZ2Z/NLNd/jY3kTXK1DTMZ++rZlblf+9tNLPrElmjTD1mNsvM/mJmO8xsm5l91j+u7z0ZV8f47J3Q956ugRuBmQWBncCVQCWwFrjZObc9oYXJtGBme4EK55wWFpVxY2aXAm3AXc65Jf6xbwKHnXPf8P9wleuc+1Ii65SpZ5jP3leBNufctxNZm0xdZlYMFDvnXjGzTGA9cCPwIfS9J+PoGJ+9d3MC33vqgRvZecBu59we51wPcA+wKsE1iYiMGefc08DhQYdXAb/07/8S738wImNqmM+eyLhyztU4517x77cCO4BS9L0n4+wYn70TogA3slLgQNx+JSfxgxY5SQ543MzWm9nqRBcj08oM51wNeP/DAYoSXI9ML7ea2WZ/iKWGscm4MbM5wHLgJfS9J6fQoM8enMD3ngLcyGyIYxp3KqfKRc65FcC1wKf94UYiIlPZT4D5wDKgBviPhFYjU5aZZQD3AZ9zzrUkuh6ZPob47J3Q954C3MgqgVlx+2VAdYJqkWnGOVftb2uBB/CG9IqcCof8sfp9Y/ZrE1yPTBPOuUPOuahzLgb8DH3vyTgwsxDeL9C/cc7d7x/W956Mu6E+eyf6vacAN7K1wAIzm2tmYeAmYE2Ca5JpwMzS/QtcMbN04Cpg67EfJTJm1gAf9O9/EHgogbXINNL3C7Tv7eh7T8aYmRnwc2CHc+47caf0vSfjarjP3ol+72kWyuPgT+X5PSAI3Omc+3piK5LpwMzm4fW6ASQBd+uzJ+PBzH4LXA4UAIeArwAPAvcC5cB+4F3OOU02IWNqmM/e5XjDiBywF/hE33VJImPBzC4GngG2ADH/8N/jXYuk7z0ZN8f47N3MCXzvKcCJiIiIiIhMEhpCKSIiIiIiMkkowImIiIiIiEwSCnAiIiIiIiKThAKciIiIiIjIJKEAJyIiIiIiMkkowImIiIiIiEwSCnAiIiIiIiKTxP8Puc7KHS4Jcn4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.716000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
